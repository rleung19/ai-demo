{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Churn Analysis - Development Notebook\n",
        "## Phase 1: Original Model (Baseline)\n",
        "\n",
        "This notebook develops an XGBoost churn prediction model using OML4Py on Oracle Autonomous Database.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Oracle Autonomous Database with OML enabled\n",
        "- ADMIN and OML schema access\n",
        "- Database objects created (LOGIN_EVENTS table, indexes, grants)\n",
        "\n",
        "**Segment Definition:**\n",
        "- VIP: Has affinity card (AFFINITY_CARD > 0)\n",
        "- Regular: 2+ orders OR $500+ spent\n",
        "- New: Exactly 1 order\n",
        "- Dormant: No orders in 2+ months\n",
        "- At-Risk: Everyone else\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create CHURN_FEATURES View\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "oml_type": "script"
      },
      "source": [
        "%script\n",
        "\n",
        "-- Step 1: Create CHURN_FEATURES View\n",
        "-- Create CHURN_FEATURES view (original version - no SUPPORT_TICKETS table)\n",
        "CREATE OR REPLACE VIEW OML.CHURN_FEATURES AS\n",
        "SELECT \n",
        "    u.ID AS USER_ID,\n",
        "    u.CUST_YEAR_OF_BIRTH,\n",
        "    u.CUST_MARITAL_STATUS,\n",
        "    u.CUST_INCOME_LEVEL,\n",
        "    u.CUST_CREDIT_LIMIT,\n",
        "    u.GENDER,\n",
        "    u.EDUCATION,\n",
        "    u.OCCUPATION,\n",
        "    u.HOUSEHOLD_SIZE,\n",
        "    u.YRS_RESIDENCE,\n",
        "    u.AFFINITY_CARD,\n",
        "    \n",
        "    -- Purchase behavior (24 months)\n",
        "    COALESCE(o_stats.ORDER_COUNT_24M, 0) AS ORDER_COUNT_24M,\n",
        "    COALESCE(o_stats.TOTAL_SPENT_24M, 0) AS TOTAL_SPENT_24M,\n",
        "    COALESCE(o_stats.AVG_ORDER_VALUE_24M, 0) AS AVG_ORDER_VALUE_24M,\n",
        "    COALESCE(o_stats.MONTHS_SINCE_LAST_PURCHASE, 999) AS MONTHS_SINCE_LAST_PURCHASE,\n",
        "    MONTHS_BETWEEN(SYSDATE, u.CREATED_AT) AS CUSTOMER_AGE_MONTHS,\n",
        "    CASE \n",
        "        WHEN COALESCE(o_stats.ORDER_COUNT_24M, 0) > 0 AND COALESCE(o_stats.MONTHS_SINCE_LAST_PURCHASE, 999) > 0 \n",
        "        THEN COALESCE(o_stats.ORDER_COUNT_24M, 0) / NULLIF(o_stats.MONTHS_SINCE_LAST_PURCHASE, 0)\n",
        "        ELSE 0\n",
        "    END AS PURCHASE_VELOCITY,\n",
        "    \n",
        "    -- Login activity (30 days)\n",
        "    COALESCE(login_stats.LOGIN_COUNT_30D, 0) AS LOGIN_COUNT_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(login_stats.LOGIN_COUNT_30D, 0) >= 15 THEN 'High'\n",
        "        WHEN COALESCE(login_stats.LOGIN_COUNT_30D, 0) >= 7 THEN 'Medium'\n",
        "        WHEN COALESCE(login_stats.LOGIN_COUNT_30D, 0) > 0 THEN 'Low'\n",
        "        ELSE 'None'\n",
        "    END AS LOGIN_FREQUENCY_CATEGORY,\n",
        "    COALESCE(login_stats.MONTHS_SINCE_LAST_LOGIN, 999) AS MONTHS_SINCE_LAST_LOGIN,\n",
        "    \n",
        "    -- Support tickets (24 months) - placeholder since table doesn't exist\n",
        "    0 AS SUPPORT_TICKETS_24M,\n",
        "    \n",
        "    -- Review and NPS (24 months)\n",
        "    COALESCE(review_stats.AVG_REVIEW_RATING, 0) AS AVG_REVIEW_RATING,\n",
        "    COALESCE(review_stats.REVIEW_COUNT, 0) AS REVIEW_COUNT,\n",
        "    COALESCE(review_stats.DETRACTOR_COUNT, 0) AS DETRACTOR_COUNT,\n",
        "    COALESCE(review_stats.PASSIVE_COUNT, 0) AS PASSIVE_COUNT,\n",
        "    COALESCE(review_stats.PROMOTER_COUNT, 0) AS PROMOTER_COUNT,\n",
        "    CASE \n",
        "        WHEN COALESCE(review_stats.REVIEW_COUNT, 0) > 0 THEN\n",
        "            ((COALESCE(review_stats.PROMOTER_COUNT, 0) - COALESCE(review_stats.DETRACTOR_COUNT, 0)) / NULLIF(review_stats.REVIEW_COUNT, 0)) * 100\n",
        "        ELSE 0\n",
        "    END AS NPS_SCORE,\n",
        "    CASE WHEN COALESCE(review_stats.NEGATIVE_REVIEWS_90D, 0) > 0 THEN 1 ELSE 0 END AS HAS_NEGATIVE_SENTIMENT,\n",
        "    COALESCE(review_stats.NEGATIVE_REVIEWS_90D, 0) AS NEGATIVE_REVIEWS_90D,\n",
        "    \n",
        "    -- Email engagement (30 days)\n",
        "    COALESCE(email_stats.EMAILS_SENT_30D, 0) AS EMAILS_SENT_30D,\n",
        "    COALESCE(email_stats.EMAILS_OPENED_30D, 0) AS EMAILS_OPENED_30D,\n",
        "    COALESCE(email_stats.EMAILS_CLICKED_30D, 0) AS EMAILS_CLICKED_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(email_stats.EMAILS_SENT_30D, 0) > 0 THEN\n",
        "            COALESCE(email_stats.EMAILS_OPENED_30D, 0) / NULLIF(email_stats.EMAILS_SENT_30D, 0)\n",
        "        ELSE 0\n",
        "    END AS EMAIL_OPEN_RATE_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(email_stats.EMAILS_OPENED_30D, 0) > 0 THEN\n",
        "            COALESCE(email_stats.EMAILS_CLICKED_30D, 0) / NULLIF(email_stats.EMAILS_OPENED_30D, 0)\n",
        "        ELSE 0\n",
        "    END AS EMAIL_CLICK_RATE_30D,\n",
        "    CASE WHEN COALESCE(email_stats.HAS_UNSUBSCRIBED, 0) > 0 THEN 1 ELSE 0 END AS HAS_UNSUBSCRIBED,\n",
        "    \n",
        "    -- Cart events (30 days)\n",
        "    COALESCE(cart_stats.CART_ADDITIONS_30D, 0) AS CART_ADDITIONS_30D,\n",
        "    COALESCE(cart_stats.TOTAL_SESSIONS_30D, 0) AS TOTAL_SESSIONS_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(cart_stats.TOTAL_SESSIONS_30D, 0) > 0 THEN\n",
        "            COALESCE(cart_stats.CART_ADDITIONS_30D, 0) / NULLIF(cart_stats.TOTAL_SESSIONS_30D, 0)\n",
        "        ELSE 0\n",
        "    END AS BROWSE_TO_CART_RATIO,\n",
        "    COALESCE(cart_stats.CART_ABANDONMENTS_30D, 0) AS CART_ABANDONMENTS_30D,\n",
        "    \n",
        "    -- Returns (24 months)\n",
        "    COALESCE(return_stats.SIZE_FIT_RETURNS_COUNT, 0) AS SIZE_FIT_RETURNS_COUNT,\n",
        "    CASE WHEN COALESCE(return_stats.SIZE_FIT_RETURNS_COUNT, 0) >= 2 THEN 1 ELSE 0 END AS HAS_2PLUS_SIZE_RETURNS,\n",
        "    COALESCE(return_stats.TOTAL_RETURNS_COUNT, 0) AS TOTAL_RETURNS_COUNT,\n",
        "    \n",
        "    -- Customer segment\n",
        "    CASE \n",
        "        WHEN COALESCE(u.AFFINITY_CARD, 0) > 0 THEN 'VIP'\n",
        "        WHEN COALESCE(o_stats.ORDER_COUNT_24M, 0) >= 2 \n",
        "             OR COALESCE(o_stats.TOTAL_SPENT_24M, 0) >= 500 THEN 'Regular'\n",
        "        WHEN COALESCE(o_stats.ORDER_COUNT_24M, 0) = 1 THEN 'New'\n",
        "        WHEN MONTHS_BETWEEN(SYSDATE, COALESCE(o_stats.LAST_PURCHASE_DATE, u.CREATED_AT)) >= 2 THEN 'Dormant'\n",
        "        ELSE 'At-Risk'\n",
        "    END AS CUSTOMER_SEGMENT,\n",
        "    \n",
        "    -- Estimated LTV\n",
        "    COALESCE(o_stats.TOTAL_SPENT_24M, 0) * 2 AS ESTIMATED_LTV\n",
        "    \n",
        "FROM ADMIN.USERS u\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        COUNT(DISTINCT ID) AS ORDER_COUNT_24M,\n",
        "        SUM(TOTAL) AS TOTAL_SPENT_24M,\n",
        "        AVG(TOTAL) AS AVG_ORDER_VALUE_24M,\n",
        "        MAX(CREATED_AT) AS LAST_PURCHASE_DATE,\n",
        "        MONTHS_BETWEEN(SYSDATE, MAX(CREATED_AT)) AS MONTHS_SINCE_LAST_PURCHASE\n",
        "    FROM ADMIN.ORDERS\n",
        "    WHERE STATUS NOT IN ('cancelled')\n",
        "      AND CREATED_AT >= ADD_MONTHS(SYSDATE, -24)\n",
        "    GROUP BY USER_ID\n",
        ") o_stats ON u.ID = o_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        COUNT(*) AS LOGIN_COUNT_30D,\n",
        "        MONTHS_BETWEEN(SYSDATE, MAX(LOGIN_TIMESTAMP)) AS MONTHS_SINCE_LAST_LOGIN\n",
        "    FROM ADMIN.LOGIN_EVENTS\n",
        "    WHERE LOGIN_TIMESTAMP >= SYSDATE - 30\n",
        "    GROUP BY USER_ID\n",
        ") login_stats ON u.ID = login_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        AVG(RATING) AS AVG_REVIEW_RATING,\n",
        "        COUNT(*) AS REVIEW_COUNT,\n",
        "        SUM(CASE WHEN RATING <= 2 THEN 1 ELSE 0 END) AS DETRACTOR_COUNT,\n",
        "        SUM(CASE WHEN RATING = 3 THEN 1 ELSE 0 END) AS PASSIVE_COUNT,\n",
        "        SUM(CASE WHEN RATING >= 4 THEN 1 ELSE 0 END) AS PROMOTER_COUNT,\n",
        "        SUM(CASE WHEN RATING <= 2 AND CREATED_AT >= SYSDATE - 90 THEN 1 ELSE 0 END) AS NEGATIVE_REVIEWS_90D\n",
        "    FROM ADMIN.PRODUCT_REVIEWS\n",
        "    WHERE CREATED_AT >= ADD_MONTHS(SYSDATE, -24)\n",
        "    GROUP BY USER_ID\n",
        ") review_stats ON u.ID = review_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        COUNT(*) AS EMAILS_SENT_30D,\n",
        "        SUM(CASE WHEN OPENED_AT IS NOT NULL THEN 1 ELSE 0 END) AS EMAILS_OPENED_30D,\n",
        "        SUM(CASE WHEN CLICKED_AT IS NOT NULL THEN 1 ELSE 0 END) AS EMAILS_CLICKED_30D,\n",
        "        SUM(CASE WHEN UNSUBSCRIBED_AT IS NOT NULL THEN 1 ELSE 0 END) AS HAS_UNSUBSCRIBED\n",
        "    FROM ADMIN.EMAIL_ENGAGEMENT\n",
        "    WHERE SENT_AT >= SYSDATE - 30\n",
        "    GROUP BY USER_ID\n",
        ") email_stats ON u.ID = email_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        SUM(CASE WHEN ACTION = 'added' THEN 1 ELSE 0 END) AS CART_ADDITIONS_30D,\n",
        "        COUNT(DISTINCT SESSION_ID) AS TOTAL_SESSIONS_30D,\n",
        "        SUM(CASE WHEN ACTION = 'abandoned' THEN 1 ELSE 0 END) AS CART_ABANDONMENTS_30D\n",
        "    FROM ADMIN.CART_EVENTS\n",
        "    WHERE CREATED_AT >= SYSDATE - 30\n",
        "    GROUP BY USER_ID\n",
        ") cart_stats ON u.ID = cart_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        SUM(CASE WHEN RETURN_REASON IN ('SIZE_TOO_SMALL', 'SIZE_TOO_LARGE') THEN 1 ELSE 0 END) AS SIZE_FIT_RETURNS_COUNT,\n",
        "        COUNT(*) AS TOTAL_RETURNS_COUNT\n",
        "    FROM ADMIN.RETURNS\n",
        "    WHERE REQUESTED_AT >= ADD_MONTHS(SYSDATE, -24)\n",
        "    GROUP BY USER_ID\n",
        ") return_stats ON u.ID = return_stats.USER_ID\n",
        "WHERE u.IS_ACTIVE = 1;\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1b: Create CHURN_TRAINING_DATA View\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "oml_type": "script"
      },
      "source": [
        "%script\n",
        "\n",
        "-- Step 1b: Create CHURN_TRAINING_DATA View\n",
        "-- Create CHURN_TRAINING_DATA view (original version with adjusted churn definition)\n",
        "CREATE OR REPLACE VIEW OML.CHURN_TRAINING_DATA AS\n",
        "SELECT \n",
        "    cf.*,\n",
        "    CASE \n",
        "        WHEN cf.CUSTOMER_SEGMENT = 'Dormant' THEN 1\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE >= 0.75 THEN 1\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE >= 0.5\n",
        "             AND (\n",
        "                 cf.LOGIN_COUNT_30D <= 7\n",
        "                 OR cf.EMAIL_OPEN_RATE_30D < 0.5\n",
        "                 OR cf.CART_ABANDONMENTS_30D >= 2\n",
        "             ) THEN 1\n",
        "        WHEN cf.LOGIN_COUNT_30D <= 3 THEN 1\n",
        "        ELSE 0\n",
        "    END AS CHURNED_60_90D\n",
        "FROM OML.CHURN_FEATURES cf\n",
        "WHERE cf.CUSTOMER_AGE_MONTHS >= 3\n",
        "    AND cf.ORDER_COUNT_24M > 0;\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1c: Verify Views\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "oml_type": "script"
      },
      "source": [
        "%script\n",
        "\n",
        "-- Step 1c: Verify Views\n",
        "-- Verify views created\n",
        "SELECT COUNT(*) AS FEATURES_COUNT FROM OML.CHURN_FEATURES;\n",
        "SELECT COUNT(*) AS TRAINING_COUNT FROM OML.CHURN_TRAINING_DATA;\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Check OML4Py Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 2: Check OML4Py Connection\n",
        "import oml\n",
        "\n",
        "# Verify connection\n",
        "print(\"OML Connected:\", oml.isconnected())\n",
        "\n",
        "# Check OML version\n",
        "try:\n",
        "    print(\"OML Version:\", oml.__version__)\n",
        "except:\n",
        "    pass\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Explore Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 3: Explore Data\n",
        "# Load features\n",
        "features = oml.sync(view='CHURN_FEATURES')\n",
        "\n",
        "# Basic info\n",
        "print(\"=\" * 60)\n",
        "print(\"CHURN_FEATURES Data Overview\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Shape:\", features.shape)\n",
        "print(\"\\nColumn count:\", len(features.columns))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3b: Check Customer Segments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 3b: Check Customer Segments\n",
        "# Check customer segments distribution\n",
        "import pandas as pd\n",
        "\n",
        "if 'CUSTOMER_SEGMENT' in features.columns:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Customer Segment Distribution\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    features_pd = features.pull()\n",
        "    segment_dist = features_pd['CUSTOMER_SEGMENT'].value_counts()\n",
        "    print(segment_dist)\n",
        "    \n",
        "    print(\"\\nPercentage distribution:\")\n",
        "    print((segment_dist / len(features_pd) * 100).round(2))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3c: Check Churn Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 3c: Check Churn Distribution\n",
        "# Check the churn distribution\n",
        "import pandas as pd\n",
        "\n",
        "train_data = oml.sync(view='CHURN_TRAINING_DATA')\n",
        "train_data_pd = train_data.pull()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Adjusted Churn Definition Results\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Training Data Shape: \" + str(train_data_pd.shape))\n",
        "print(\"\\nTarget Variable Distribution:\")\n",
        "print(train_data_pd['CHURNED_60_90D'].value_counts())\n",
        "churn_rate = train_data_pd['CHURNED_60_90D'].mean() * 100\n",
        "print(\"\\nChurn Rate: \" + str(churn_rate) + \" %\")\n",
        "print(\"Non-Churn Rate: \" + str(100 - churn_rate) + \" %\")\n",
        "print(\"\\n✓ Good churn rate for training!\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare Features and Split Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 4: Prepare Features and Split Data\n",
        "# Prepare features and split data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load training data\n",
        "train_data = oml.sync(view='CHURN_TRAINING_DATA')\n",
        "train_data_pd = train_data.pull()\n",
        "\n",
        "# Identify feature columns (exclude target and metadata)\n",
        "exclude_cols = ['USER_ID', 'CUSTOMER_SEGMENT', 'ESTIMATED_LTV', 'CHURNED_60_90D']\n",
        "feature_cols = [col for col in train_data_pd.columns if col not in exclude_cols]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Preparing Features and Splitting Data\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Total features: \" + str(len(feature_cols)))\n",
        "print(\"Features: \" + \", \".join(feature_cols[:10]) + \"...\")\n",
        "\n",
        "# Prepare X and y\n",
        "X_pd = train_data_pd[feature_cols].copy()\n",
        "y_pd = train_data_pd['CHURNED_60_90D']\n",
        "\n",
        "# Clean data - replace NaN and infinity\n",
        "for col in feature_cols:\n",
        "    if pd.api.types.is_numeric_dtype(X_pd[col]):\n",
        "        X_pd[col] = X_pd[col].replace([np.inf, -np.inf], np.nan)\n",
        "        X_pd[col] = X_pd[col].fillna(0)\n",
        "\n",
        "# Stratified split\n",
        "X_train_pd, X_test_pd, y_train_pd, y_test_pd = train_test_split(\n",
        "    X_pd, y_pd, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_pd\n",
        ")\n",
        "\n",
        "print(\"\\nSplit completed:\")\n",
        "print(\"  Train size: \" + str(len(X_train_pd)))\n",
        "print(\"  Test size: \" + str(len(X_test_pd)))\n",
        "print(\"  Train churn rate: \" + str(round(y_train_pd.mean() * 100, 2)) + \"%\")\n",
        "print(\"  Test churn rate: \" + str(round(y_test_pd.mean() * 100, 2)) + \"%\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train XGBoost Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 5: Train XGBoost Model\n",
        "# Train XGBoost - Simplified approach\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training XGBoost Model\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Merge X_train and y_train for database push\n",
        "train_combined_pd = X_train_pd.copy()\n",
        "train_combined_pd['CHURNED_60_90D'] = y_train_pd.values\n",
        "\n",
        "# Push to database\n",
        "print(\"Pushing training data to database...\")\n",
        "train_oml = oml.push(train_combined_pd)\n",
        "print(\"Training data pushed: \" + str(train_oml.shape))\n",
        "\n",
        "# Create XGBoost model with explicit classification type\n",
        "xgb_model = oml.xgb('classification')\n",
        "\n",
        "# Get features and target from OML DataFrame\n",
        "X_train_oml = train_oml[feature_cols]\n",
        "y_train_oml = train_oml['CHURNED_60_90D']\n",
        "\n",
        "print(\"X_train_oml shape: \" + str(X_train_oml.shape))\n",
        "print(\"Training started...\")\n",
        "\n",
        "# Fit the model\n",
        "xgb_model = xgb_model.fit(X_train_oml, y_train_oml)\n",
        "print(\"Training completed!\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 6: Evaluate Model\n",
        "# Evaluate model performance\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Prepare test data in OML format\n",
        "test_combined_pd = X_test_pd.copy()\n",
        "test_combined_pd['CHURNED_60_90D'] = y_test_pd.values\n",
        "test_oml = oml.push(test_combined_pd)\n",
        "X_test_oml = test_oml[feature_cols]\n",
        "\n",
        "# Get predictions - OML returns Vector objects\n",
        "print(\"Generating predictions...\")\n",
        "y_pred_proba_oml = xgb_model.predict_proba(X_test_oml)\n",
        "\n",
        "# Convert OML Vector to numpy array\n",
        "y_pred_proba_pd = y_pred_proba_oml.pull()\n",
        "if isinstance(y_pred_proba_pd, pd.DataFrame):\n",
        "    if 1 in y_pred_proba_pd.columns:\n",
        "        y_pred_proba = y_pred_proba_pd[1].values\n",
        "    elif len(y_pred_proba_pd.columns) == 2:\n",
        "        y_pred_proba = y_pred_proba_pd.iloc[:, 1].values\n",
        "    else:\n",
        "        y_pred_proba = y_pred_proba_pd.values.flatten()\n",
        "else:\n",
        "    y_pred_proba = np.array(y_pred_proba_pd)\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# Convert y_test to numpy array for metrics\n",
        "y_test_vals = y_test_pd.values\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test_vals, y_pred)\n",
        "precision = precision_score(y_test_vals, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test_vals, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test_vals, y_pred, zero_division=0)\n",
        "auc = roc_auc_score(y_test_vals, y_pred_proba)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_vals, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Accuracy:  \" + str(round(accuracy, 4)))\n",
        "print(\"Precision: \" + str(round(precision, 4)) + \"  (of predicted churn, how many actually churn)\")\n",
        "print(\"Recall:    \" + str(round(recall, 4)) + \"  (of actual churn, how many did we catch)\")\n",
        "print(\"F1 Score:  \" + str(round(f1, 4)) + \"  (harmonic mean of precision and recall)\")\n",
        "print(\"AUC-ROC:   \" + str(round(auc, 4)) + \"  (model's ability to distinguish classes)\")\n",
        "print(\"\\nModel Confidence: \" + str(int(auc * 100)) + \"%\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"                Predicted\")\n",
        "print(\"              Non-Churn  Churn\")\n",
        "print(\"Actual Non-Churn   \" + str(tn) + \"   \" + str(fp))\n",
        "print(\"       Churn       \" + str(fn) + \"   \" + str(tp))\n",
        "print(\"\\nTrue Negatives:  \" + str(tn) + \" (correctly predicted non-churn)\")\n",
        "print(\"False Positives: \" + str(fp) + \" (predicted churn, but didn't churn)\")\n",
        "print(\"False Negatives: \" + str(fn) + \" (missed churners)\")\n",
        "print(\"True Positives:  \" + str(tp) + \" (correctly predicted churn)\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_vals, y_pred, target_names=['Non-Churn', 'Churn']))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 7: Feature Importance\n",
        "# Get and display feature importance - OML-compatible\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Feature Importance Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get importance from model (it's a property, not a method!)\n",
        "try:\n",
        "    print(\"\\nExtracting feature importance from xgb_model.importance...\")\n",
        "    importance_result = xgb_model.importance  # Property, not method\n",
        "    \n",
        "    # Pull OML DataFrame to pandas\n",
        "    if hasattr(importance_result, 'pull'):\n",
        "        importance_df_raw = importance_result.pull()\n",
        "    else:\n",
        "        importance_df_raw = importance_result\n",
        "    \n",
        "    # OML XGBoost importance has columns: ATTRIBUTE_NAME and GAIN\n",
        "    if 'ATTRIBUTE_NAME' in importance_df_raw.columns and 'GAIN' in importance_df_raw.columns:\n",
        "        feature_importance = importance_df_raw[['ATTRIBUTE_NAME', 'GAIN']].copy()\n",
        "        feature_importance.columns = ['FEATURE_NAME', 'IMPORTANCE_SCORE']\n",
        "        importance_method = \"xgb_model.importance - GAIN metric\"\n",
        "        \n",
        "        # Sort by importance\n",
        "        importance_df = feature_importance.sort_values('IMPORTANCE_SCORE', ascending=False)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Top 20 Most Important Features (by GAIN)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(importance_df.head(20).to_string(index=False))\n",
        "        \n",
        "        # Analyze feature importance distribution\n",
        "        print(\"\\nTotal features with importance data: \" + str(len(importance_df)))\n",
        "        positive_importance = importance_df[importance_df['IMPORTANCE_SCORE'] > 0]\n",
        "        print(\"Features with positive importance: \" + str(len(positive_importance)))\n",
        "        \n",
        "        if len(positive_importance) > 0:\n",
        "            total_importance = importance_df['IMPORTANCE_SCORE'].sum()\n",
        "            if abs(total_importance) > 1e-10:\n",
        "                top10_pct = importance_df.head(10)['IMPORTANCE_SCORE'].sum() / total_importance * 100\n",
        "                top5_pct = importance_df.head(5)['IMPORTANCE_SCORE'].sum() / total_importance * 100\n",
        "                print(\"Top 5 features account for: \" + str(round(top5_pct, 1)) + \"% of total importance\")\n",
        "                print(\"Top 10 features account for: \" + str(round(top10_pct, 1)) + \"% of total importance\")\n",
        "        \n",
        "        # Store for later use\n",
        "        importance_df_result = importance_df\n",
        "    else:\n",
        "        print(\"Could not find expected columns in importance data\")\n",
        "        importance_df_result = None\n",
        "        \n",
        "except Exception as e:\n",
        "    print(\"Failed to extract importance: \" + str(e))\n",
        "    importance_df_result = None\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 8: Save Model\n",
        "# Save model to OML datastore\n",
        "model_name = 'CHURN_XGBOOST_MODEL'\n",
        "\n",
        "# Save - correct syntax: first arg is dict of objects, then name\n",
        "oml.ds.save({'churn_xgb_model': xgb_model}, model_name, description='Churn XGBoost Model v1', overwrite=True)\n",
        "print(\"✓ Model '\" + model_name + \"' saved to OML datastore\")\n",
        "\n",
        "# Verify model is persisted\n",
        "print(\"✓ Model is persisted in Oracle Database\")\n",
        "print(\"Model type: \" + str(type(xgb_model)))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Optimize Threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 9: Optimize Threshold\n",
        "# Find optimal probability threshold\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Use predictions from Step 6 (y_test_vals and y_pred_proba should be available)\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "f1_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "accuracies = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test_vals, y_pred_thresh, zero_division=0)\n",
        "    prec = precision_score(y_test_vals, y_pred_thresh, zero_division=0)\n",
        "    rec = recall_score(y_test_vals, y_pred_thresh, zero_division=0)\n",
        "    acc = accuracy_score(y_test_vals, y_pred_thresh)\n",
        "    f1_scores.append(f1)\n",
        "    precisions.append(prec)\n",
        "    recalls.append(rec)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Find optimal threshold (maximize F1)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"THRESHOLD OPTIMIZATION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimal Threshold: \" + str(round(optimal_threshold, 3)) + \" (instead of 0.5)\")\n",
        "print(\"F1 Score at optimal: \" + str(round(f1_scores[optimal_idx], 4)))\n",
        "print(\"Precision at optimal: \" + str(round(precisions[optimal_idx], 4)))\n",
        "print(\"Recall at optimal: \" + str(round(recalls[optimal_idx], 4)))\n",
        "print(\"Accuracy at optimal: \" + str(round(accuracies[optimal_idx], 4)))\n",
        "\n",
        "# Re-evaluate with optimal threshold\n",
        "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "accuracy_opt = accuracy_score(y_test_vals, y_pred_optimal)\n",
        "precision_opt = precision_score(y_test_vals, y_pred_optimal, zero_division=0)\n",
        "recall_opt = recall_score(y_test_vals, y_pred_optimal, zero_division=0)\n",
        "f1_opt = f1_score(y_test_vals, y_pred_optimal, zero_division=0)\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(\"  Threshold 0.5:  F1=\" + str(round(f1, 4)) + \", Precision=\" + str(round(precision, 4)) + \", Recall=\" + str(round(recall, 4)))\n",
        "print(\"  Threshold \" + str(round(optimal_threshold, 3)) + \": F1=\" + str(round(f1_opt, 4)) + \", Precision=\" + str(round(precision_opt, 4)) + \", Recall=\" + str(round(recall_opt, 4)))\n",
        "\n",
        "# Store optimal threshold for later use\n",
        "optimal_threshold_value = optimal_threshold\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Score All Customers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 10: Score All Customers\n",
        "# Score all active customers for churn risk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load current features (all active customers)\n",
        "current_features = oml.sync(view='CHURN_FEATURES')\n",
        "current_features_pd = current_features.pull()\n",
        "\n",
        "# Prepare features\n",
        "X_current_pd = current_features_pd[feature_cols].copy()\n",
        "\n",
        "# Clean data before pushing\n",
        "print(\"Cleaning data before scoring...\")\n",
        "X_current_pd = X_current_pd.replace([np.inf, -np.inf], np.nan)\n",
        "numeric_cols = X_current_pd.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    X_current_pd[col] = pd.to_numeric(X_current_pd[col], errors='coerce').fillna(0)\n",
        "\n",
        "# Push to OML for scoring\n",
        "X_current_oml = oml.push(X_current_pd)\n",
        "\n",
        "# Predict churn probability\n",
        "print(\"=\" * 60)\n",
        "print(\"Scoring All Customers\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Scoring \" + str(len(X_current_pd)) + \" customers...\")\n",
        "churn_proba_oml = xgb_model.predict_proba(X_current_oml)\n",
        "\n",
        "# Convert to numpy array\n",
        "churn_proba_pd = churn_proba_oml.pull()\n",
        "if isinstance(churn_proba_pd, pd.DataFrame):\n",
        "    if 1 in churn_proba_pd.columns:\n",
        "        churn_proba = churn_proba_pd[1].values\n",
        "    elif len(churn_proba_pd.columns) == 2:\n",
        "        churn_proba = churn_proba_pd.iloc[:, 1].values\n",
        "    else:\n",
        "        churn_proba = churn_proba_pd.values.flatten()\n",
        "else:\n",
        "    churn_proba = np.array(churn_proba_pd)\n",
        "\n",
        "# Use optimal threshold (from Step 9) or default to 0.3 if not available\n",
        "threshold = optimal_threshold_value if 'optimal_threshold_value' in globals() else 0.3\n",
        "\n",
        "# Create results DataFrame\n",
        "results = pd.DataFrame({\n",
        "    'USER_ID': current_features_pd['USER_ID'],\n",
        "    'CHURN_RISK_SCORE': churn_proba,\n",
        "    'CHURN_RISK_PERCENT': (churn_proba * 100).round(2),\n",
        "    'IS_AT_RISK': (churn_proba >= threshold).astype(int),\n",
        "    'CUSTOMER_SEGMENT': current_features_pd['CUSTOMER_SEGMENT'],\n",
        "    'ESTIMATED_LTV': current_features_pd['ESTIMATED_LTV']\n",
        "})\n",
        "\n",
        "results['LTV_AT_RISK'] = (results['CHURN_RISK_PERCENT'] / 100 * results['ESTIMATED_LTV']).round(2)\n",
        "\n",
        "# Calculate summary statistics\n",
        "total_customers = len(results)\n",
        "at_risk_count = int(results['IS_AT_RISK'].sum())\n",
        "avg_risk = float(results['CHURN_RISK_PERCENT'].mean())\n",
        "median_risk = float(results['CHURN_RISK_PERCENT'].median())\n",
        "total_ltv_at_risk = float(results['LTV_AT_RISK'].sum())\n",
        "\n",
        "# Risk distribution counts\n",
        "low_risk_count = int((results['CHURN_RISK_PERCENT'] < 30).sum())\n",
        "medium_risk_count = int(((results['CHURN_RISK_PERCENT'] >= 30) & (results['CHURN_RISK_PERCENT'] < 50)).sum())\n",
        "high_risk_count = int(((results['CHURN_RISK_PERCENT'] >= 50) & (results['CHURN_RISK_PERCENT'] < 70)).sum())\n",
        "very_high_risk_count = int((results['CHURN_RISK_PERCENT'] >= 70).sum())\n",
        "\n",
        "# Print results\n",
        "print(\"\\nScoring Results (Threshold: \" + str(round(threshold, 3)) + \"):\")\n",
        "print(\"Total customers scored: \" + str(total_customers))\n",
        "print(\"At-risk customers (>=\" + str(int(threshold*100)) + \"% risk): \" + str(at_risk_count))\n",
        "print(\"Average risk score: \" + str(round(avg_risk, 2)) + \"%\")\n",
        "print(\"Median risk score: \" + str(round(median_risk, 2)) + \"%\")\n",
        "print(\"Total LTV at risk: $\" + format(total_ltv_at_risk, ',.2f'))\n",
        "\n",
        "print(\"\\nRisk Score Distribution:\")\n",
        "print(\"  Low risk (<30%): \" + str(low_risk_count))\n",
        "print(\"  Medium risk (30-50%): \" + str(medium_risk_count))\n",
        "print(\"  High risk (50-70%): \" + str(high_risk_count))\n",
        "print(\"  Very high risk (>=70%): \" + str(very_high_risk_count))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Cohort-Level Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 11: Cohort-Level Metrics\n",
        "# Analyze metrics by customer segment (cohort)\n",
        "import pandas as pd\n",
        "\n",
        "# Cohort analysis using pandas groupby\n",
        "cohort_metrics = results.groupby('CUSTOMER_SEGMENT').agg({\n",
        "    'USER_ID': 'count',\n",
        "    'CHURN_RISK_PERCENT': 'mean',\n",
        "    'ESTIMATED_LTV': 'sum',\n",
        "    'LTV_AT_RISK': 'sum',\n",
        "    'IS_AT_RISK': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "cohort_metrics.columns = [\n",
        "    'COHORT_NAME', \n",
        "    'CUSTOMER_COUNT', \n",
        "    'AVG_RISK_SCORE', \n",
        "    'TOTAL_LTV', \n",
        "    'LTV_AT_RISK', \n",
        "    'AT_RISK_COUNT'\n",
        "]\n",
        "\n",
        "# Calculate additional metrics\n",
        "cohort_metrics['AT_RISK_PERCENT'] = (cohort_metrics['AT_RISK_COUNT'] / cohort_metrics['CUSTOMER_COUNT'] * 100).round(2)\n",
        "cohort_metrics['LTV_RISK_PERCENT'] = (cohort_metrics['LTV_AT_RISK'] / cohort_metrics['TOTAL_LTV'] * 100).round(2)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Cohort-Level Metrics\")\n",
        "print(\"=\" * 60)\n",
        "print(cohort_metrics.to_string(index=False))\n",
        "\n",
        "# Display summary insights\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Key Insights by Cohort\")\n",
        "print(\"=\" * 60)\n",
        "for _, row in cohort_metrics.iterrows():\n",
        "    print(\"\\n\" + str(row['COHORT_NAME']) + \":\")\n",
        "    print(\"  - \" + str(row['CUSTOMER_COUNT']) + \" customers\")\n",
        "    print(\"  - Avg Risk Score: \" + str(round(row['AVG_RISK_SCORE'], 1)) + \"%\")\n",
        "    print(\"  - At-Risk Count: \" + str(row['AT_RISK_COUNT']) + \" (\" + str(row['AT_RISK_PERCENT']) + \"%)\")\n",
        "    print(\"  - LTV at Risk: $\" + format(row['LTV_AT_RISK'], ',.0f') + \" (\" + str(row['LTV_RISK_PERCENT']) + \"% of total LTV)\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Analyze Risk Factors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 12: Analyze Risk Factors\n",
        "# Analyze top risk factors driving churn predictions\n",
        "import pandas as pd\n",
        "\n",
        "# Merge results with features\n",
        "merged = results.merge(current_features_pd, on='USER_ID', how='inner', suffixes=('', '_y'))\n",
        "\n",
        "# Check if CUSTOMER_SEGMENT exists, if not get it from results\n",
        "if 'CUSTOMER_SEGMENT' not in merged.columns:\n",
        "    if 'CUSTOMER_SEGMENT' in results.columns:\n",
        "        merged['CUSTOMER_SEGMENT'] = results['CUSTOMER_SEGMENT']\n",
        "    elif 'CUSTOMER_SEGMENT_y' in merged.columns:\n",
        "        merged['CUSTOMER_SEGMENT'] = merged['CUSTOMER_SEGMENT_y']\n",
        "\n",
        "# Analyze each risk factor based on top important features\n",
        "risk_factors = []\n",
        "\n",
        "# 1. Months Since Last Purchase (Top feature - 77% importance)\n",
        "high_months = merged[merged['MONTHS_SINCE_LAST_PURCHASE'] >= 1.5]\n",
        "if len(high_months) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in high_months.columns and len(high_months['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = high_months['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'No Purchase in 45+ Days (1.5+ months)',\n",
        "        'IMPACT_SCORE': str(int(high_months['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(high_months),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(high_months['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': '77%'\n",
        "    })\n",
        "\n",
        "# 2. Cart Abandonments (11.9% importance)\n",
        "high_abandon = merged[merged['CART_ABANDONMENTS_30D'] >= 3]\n",
        "if len(high_abandon) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in high_abandon.columns and len(high_abandon['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = high_abandon['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'High Cart Abandonments (3+)',\n",
        "        'IMPACT_SCORE': str(int(high_abandon['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(high_abandon),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(high_abandon['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': '11.9%'\n",
        "    })\n",
        "\n",
        "# 3. Low Login Activity (10.8% importance)\n",
        "low_login = merged[merged['LOGIN_COUNT_30D'] <= 3]\n",
        "if len(low_login) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in low_login.columns and len(low_login['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = low_login['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'Low Login Activity (≤3 logins)',\n",
        "        'IMPACT_SCORE': str(int(low_login['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(low_login),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(low_login['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': '10.8%'\n",
        "    })\n",
        "\n",
        "# 4. Email Engagement\n",
        "low_email = merged[merged['EMAIL_OPEN_RATE_30D'] < 0.2]\n",
        "if len(low_email) > 0:\n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'Email Engagement Decay',\n",
        "        'IMPACT_SCORE': str(int(low_email['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(low_email),\n",
        "        'PRIMARY_SEGMENT': 'All segments',\n",
        "        'AVG_RISK': float(low_email['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': 'Low'\n",
        "    })\n",
        "\n",
        "# 5. Size/Fit Returns\n",
        "size_returns = merged[merged['HAS_2PLUS_SIZE_RETURNS'] == 1]\n",
        "if len(size_returns) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in size_returns.columns and len(size_returns['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = size_returns['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'Size/Fit Issues (2+ returns)',\n",
        "        'IMPACT_SCORE': str(int(size_returns['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(size_returns),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(size_returns['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': 'Low'\n",
        "    })\n",
        "\n",
        "# Create DataFrame and sort by average risk\n",
        "risk_factors_df = pd.DataFrame(risk_factors)\n",
        "if len(risk_factors_df) > 0:\n",
        "    risk_factors_df = risk_factors_df.sort_values('AVG_RISK', ascending=False)\n",
        "    risk_factors_display = risk_factors_df[['RISK_FACTOR', 'IMPACT_SCORE', 'AFFECTED_CUSTOMERS', 'PRIMARY_SEGMENT', 'FEATURE_IMPORTANCE']]\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Top Risk Factors (Based on Model's Important Features)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(risk_factors_display.to_string(index=False))\n",
        "else:\n",
        "    print(\"No risk factors identified\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Summary Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%python\n",
        "\n",
        "# Step 13: Summary Report\n",
        "# Generate comprehensive summary report\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHURN MODEL DEVELOPMENT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nModel Name: CHURN_XGBOOST_MODEL\")\n",
        "print(\"Model Type: XGBoost Binary Classification\")\n",
        "print(\"Churn Definition: Multi-factor (Dormant, 0.75+ months, activity decline, low login)\")\n",
        "\n",
        "print(\"\\n--- Training Data ---\")\n",
        "print(\"Training Samples: \" + str(len(X_train_pd)))\n",
        "print(\"Test Samples: \" + str(len(X_test_pd)))\n",
        "print(\"Total Features: \" + str(len(feature_cols)))\n",
        "if 'importance_df_result' in globals():\n",
        "    print(\"Features Used by Model: \" + str(len(importance_df_result)))\n",
        "else:\n",
        "    print(\"Features Used by Model: N/A\")\n",
        "\n",
        "print(\"\\n--- Model Performance (Threshold 0.5) ---\")\n",
        "print(\"  Accuracy:  \" + str(round(accuracy, 4)) + \" (\" + str(round(accuracy*100, 2)) + \"%)\")\n",
        "print(\"  Precision: \" + str(round(precision, 4)) + \" (\" + str(round(precision*100, 2)) + \"%)\")\n",
        "print(\"  Recall:    \" + str(round(recall, 4)) + \" (\" + str(round(recall*100, 2)) + \"%)\")\n",
        "print(\"  F1 Score:  \" + str(round(f1, 4)))\n",
        "print(\"  AUC-ROC:   \" + str(round(auc, 4)))\n",
        "print(\"  Model Confidence: \" + str(int(auc * 100)) + \"%\")\n",
        "\n",
        "if 'optimal_threshold_value' in globals():\n",
        "    print(\"\\n--- Model Performance (Optimal Threshold \" + str(round(optimal_threshold_value, 3)) + \") ---\")\n",
        "    print(\"  Accuracy:  \" + str(round(accuracy_opt, 4)) + \" (\" + str(round(accuracy_opt*100, 2)) + \"%)\")\n",
        "    print(\"  Precision: \" + str(round(precision_opt, 4)) + \" (\" + str(round(precision_opt*100, 2)) + \"%)\")\n",
        "    print(\"  Recall:    \" + str(round(recall_opt, 4)) + \" (\" + str(round(recall_opt*100, 2)) + \"%)\")\n",
        "    print(\"  F1 Score:  \" + str(round(f1_opt, 4)))\n",
        "\n",
        "print(\"\\n--- Top 5 Most Important Features ---\")\n",
        "if 'importance_df_result' in globals():\n",
        "    top5 = importance_df_result.head(5)\n",
        "    total_importance = importance_df_result['IMPORTANCE_SCORE'].sum()\n",
        "    for idx, row in top5.iterrows():\n",
        "        pct = (row['IMPORTANCE_SCORE'] / total_importance * 100) if total_importance > 0 else 0\n",
        "        print(\"  \" + str(row['FEATURE_NAME']) + \": \" + str(round(row['IMPORTANCE_SCORE'], 4)) + \" (\" + str(round(pct, 1)) + \"%)\")\n",
        "\n",
        "print(\"\\n--- Scoring Results ---\")\n",
        "threshold = optimal_threshold_value if 'optimal_threshold_value' in globals() else 0.3\n",
        "print(\"  Total Customers Scored: \" + str(len(results)))\n",
        "print(\"  At-Risk Customers (Threshold: \" + str(round(threshold, 3)) + \"): \" + str(int(results['IS_AT_RISK'].sum())))\n",
        "print(\"  Average Risk Score: \" + str(round(results['CHURN_RISK_PERCENT'].mean(), 2)) + \"%\")\n",
        "print(\"  Total LTV at Risk: $\" + format(results['LTV_AT_RISK'].sum(), ',.2f'))\n",
        "\n",
        "print(\"\\n--- Cohort Summary ---\")\n",
        "for _, row in cohort_metrics.iterrows():\n",
        "    print(\"  \" + str(row['COHORT_NAME']) + \": \" + str(row['AT_RISK_COUNT']) + \" at-risk (\" + str(row['AT_RISK_PERCENT']) + \"%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL PERFORMANCE NOTES\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nCurrent Issues:\")\n",
        "print(\"  1. AUC-ROC is low (52%) - barely better than random\")\n",
        "print(\"  2. Precision and Recall are both low (18-20%)\")\n",
        "print(\"  3. Model heavily relies on 3 features (99.9% of importance)\")\n",
        "\n",
        "print(\"\\nRecommended Next Steps:\")\n",
        "print(\"  1. Feature Engineering:\")\n",
        "print(\"     - Model only using 10 features - check if others are being filtered\")\n",
        "print(\"     - Create interaction features (e.g., months_since_purchase * login_count)\")\n",
        "print(\"  2. Hyperparameter Tuning:\")\n",
        "print(\"     - Adjust max_depth, learning_rate, n_estimators\")\n",
        "print(\"     - Try different scale_pos_weight values\")\n",
        "print(\"  3. Churn Definition:\")\n",
        "print(\"     - Current definition may not align with actual churn behavior\")\n",
        "print(\"     - Consider validating churn definition with business stakeholders\")\n",
        "print(\"  4. Data Quality:\")\n",
        "print(\"     - Check if feature distributions make sense\")\n",
        "print(\"     - Verify data quality in top 3 important features\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model development and evaluation complete!\")\n",
        "print(\"=\" * 60)\n"
      ],
      "outputs": []
    }
  ]
}
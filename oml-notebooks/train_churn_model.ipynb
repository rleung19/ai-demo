{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Churn Prediction Model Training\n",
        "\n",
        "This notebook trains a churn prediction model using OML4Py XGBoost.\n",
        "\n",
        "## Steps:\n",
        "1. Load training data from CHURN_TRAINING_DATA view\n",
        "2. Split data into train/test sets\n",
        "3. Train XGBoost model using OML4Py\n",
        "4. Evaluate model performance\n",
        "5. Save model to OML Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Cell 1: Import and Setup\n",
        "import oml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Churn Prediction Model Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Check OML connection\n",
        "if oml.isconnected():\n",
        "    print(\"‚úì OML connected\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  OML not connected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Cell 2: Load Training Data\n",
        "\n",
        "# Load data from view\n",
        "print(\"Loading data from CHURN_TRAINING_DATA view...\")\n",
        "train_data_oml = oml.sync(view='CHURN_TRAINING_DATA')\n",
        "train_data_pd = train_data_oml.pull()\n",
        "\n",
        "print(f\"‚úì Loaded {len(train_data_pd):,} rows\")\n",
        "print(f\"‚úì Columns: {len(train_data_pd.columns)}\")\n",
        "\n",
        "# Check churn distribution\n",
        "churn_rate = train_data_pd['CHURNED'].mean()\n",
        "print(f\"\\nData Quality Check:\")\n",
        "print(f\"  Churn rate: {churn_rate*100:.2f}%\")\n",
        "print(f\"  Churned: {train_data_pd['CHURNED'].sum():,}\")\n",
        "print(f\"  Retained: {(train_data_pd['CHURNED'] == 0).sum():,}\")\n",
        "\n",
        "# Identify feature columns\n",
        "exclude_cols = ['USER_ID', 'CHURNED']\n",
        "feature_cols = [col for col in train_data_pd.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"\\n‚úì Feature columns: {len(feature_cols)}\")\n",
        "print(f\"  Features: {', '.join(feature_cols[:10])}{'...' if len(feature_cols) > 10 else ''}\")\n",
        "\n",
        "# Prepare X and y\n",
        "X_pd = train_data_pd[feature_cols].copy()\n",
        "y_pd = train_data_pd['CHURNED'].copy()\n",
        "\n",
        "# Clean data - use robust type conversion like validation script\n",
        "print(\"\\nCleaning data...\")\n",
        "for col in feature_cols:\n",
        "    if pd.api.types.is_numeric_dtype(X_pd[col]):\n",
        "        # Replace infinity and NaN\n",
        "        X_pd[col] = X_pd[col].replace([np.inf, -np.inf], np.nan)\n",
        "        # Use pd.to_numeric with errors='coerce' for robust conversion\n",
        "        X_pd[col] = pd.to_numeric(X_pd[col], errors='coerce').fillna(0)\n",
        "    elif pd.api.types.is_object_dtype(X_pd[col]):\n",
        "        # Handle categorical columns - convert to numeric if needed\n",
        "        X_pd[col] = pd.Categorical(X_pd[col]).codes\n",
        "        # Ensure numeric\n",
        "        X_pd[col] = pd.to_numeric(X_pd[col], errors='coerce').fillna(0)\n",
        "\n",
        "# Check for constant columns (no variance)\n",
        "constant_cols = []\n",
        "for col in feature_cols:\n",
        "    if X_pd[col].nunique() <= 1:\n",
        "        constant_cols.append(col)\n",
        "\n",
        "if constant_cols:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Found {len(constant_cols)} constant columns: {constant_cols}\")\n",
        "    feature_cols = [col for col in feature_cols if col not in constant_cols]\n",
        "    X_pd = X_pd[feature_cols]\n",
        "    print(f\"  Removed constant columns, using {len(feature_cols)} features\")\n",
        "\n",
        "print(\"‚úì Data cleaned\")\n",
        "print(f\"  Final feature count: {len(feature_cols)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Cell 2.5: Data Quality Diagnostics (Optional)\n",
        "# Run this cell if model performance is poor to diagnose issues\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Data Quality Diagnostics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check feature distributions\n",
        "print(\"\\nFeature Statistics (First 5 Features):\")\n",
        "print(\"-\" * 60)\n",
        "for col in feature_cols[:5]:  # Check first 5 features\n",
        "    if col in X_pd.columns and pd.api.types.is_numeric_dtype(X_pd[col]):\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Mean: {X_pd[col].mean():.2f}\")\n",
        "        print(f\"  Std:  {X_pd[col].std():.2f}\")\n",
        "        print(f\"  Min:  {X_pd[col].min():.2f}\")\n",
        "        print(f\"  Max:  {X_pd[col].max():.2f}\")\n",
        "        print(f\"  NaN:  {X_pd[col].isna().sum()}\")\n",
        "    elif col in X_pd.columns:\n",
        "        print(f\"\\n{col}: (non-numeric)\")\n",
        "        print(f\"  Unique values: {X_pd[col].nunique()}\")\n",
        "\n",
        "# Check correlation with target\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Feature-Target Correlations (Top 10)\")\n",
        "print(\"=\" * 60)\n",
        "correlations = []\n",
        "for col in feature_cols:\n",
        "    if col in X_pd.columns and pd.api.types.is_numeric_dtype(X_pd[col]):\n",
        "        try:\n",
        "            corr = X_pd[col].corr(y_pd)\n",
        "            if not np.isnan(corr):\n",
        "                correlations.append((col, abs(corr)))\n",
        "        except Exception:\n",
        "            pass  # Skip if correlation calculation fails\n",
        "\n",
        "if len(correlations) > 0:\n",
        "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
        "    print(\"\\nTop correlated features:\")\n",
        "    for col, corr in correlations[:10]:\n",
        "        print(f\"  {col:30} {corr:.4f}\")\n",
        "    \n",
        "    max_corr = max([c[1] for c in correlations])\n",
        "    if max_corr < 0.1:\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: Very low correlations with target!\")\n",
        "        print(f\"   Maximum correlation: {max_corr:.4f}\")\n",
        "        print(\"   Features may not be predictive of churn.\")\n",
        "        print(\"   This could explain poor model performance.\")\n",
        "    else:\n",
        "        print(f\"\\n‚úì Maximum correlation: {max_corr:.4f}\")\n",
        "        if max_corr > 0.3:\n",
        "            print(\"   Good: Some features are strongly correlated with churn\")\n",
        "        elif max_corr > 0.1:\n",
        "            print(\"   Moderate: Features have some predictive power\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Could not calculate correlations!\")\n",
        "    print(\"   Check data types and ensure features are numeric\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Cell 3: Split Data\n",
        "# Split into train/test\n",
        "X_train_pd, X_test_pd, y_train_pd, y_test_pd = train_test_split(\n",
        "    X_pd, y_pd,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_pd\n",
        ")\n",
        "\n",
        "print(f\"‚úì Train size: {len(X_train_pd):,}\")\n",
        "print(f\"‚úì Test size: {len(X_test_pd):,}\")\n",
        "print(f\"‚úì Train churn rate: {y_train_pd.mean() * 100:.2f}%\")\n",
        "print(f\"‚úì Test churn rate: {y_test_pd.mean() * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4.5: REMOVED\n",
        "# This diagnostic cell was removed to avoid confusion about execution order.\n",
        "# The notebook now follows a clear sequence: Cell 1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Cell 4: Train RandomForest Model\n",
        "# Merge X_train and y_train for database push\n",
        "train_combined_pd = X_train_pd.copy()\n",
        "train_combined_pd['CHURNED'] = y_train_pd.values\n",
        "\n",
        "# CRITICAL: Ensure all numeric columns are float64 to prevent data type corruption\n",
        "# OML push can convert float64 to int64, losing precision and causing data corruption\n",
        "print(\"Ensuring data types are preserved...\")\n",
        "for col in feature_cols:\n",
        "    if col in train_combined_pd.columns:\n",
        "        if pd.api.types.is_numeric_dtype(train_combined_pd[col]):\n",
        "            # Force float64 to prevent int64 conversion during OML push\n",
        "            train_combined_pd[col] = train_combined_pd[col].astype('float64')\n",
        "print(\"‚úì Data types standardized to float64\")\n",
        "\n",
        "# Push to database\n",
        "print(\"\\nPushing training data to OML...\")\n",
        "train_oml = oml.push(train_combined_pd)\n",
        "print(f\"‚úì Training data pushed: {train_oml.shape}\")\n",
        "\n",
        "# Get features and target\n",
        "X_train_oml = train_oml[feature_cols]\n",
        "y_train_oml = train_oml['CHURNED']\n",
        "\n",
        "print(f\"‚úì X_train_oml shape: {X_train_oml.shape}\")\n",
        "print(f\"‚úì y_train_oml shape: {y_train_oml.shape}\")\n",
        "print(f\"‚úì Training churn rate: {y_train_oml.mean()*100:.2f}%\")\n",
        "\n",
        "# Verify data quality\n",
        "print(f\"\\nData Quality Check:\")\n",
        "# Pull a sample to check data quality (OML DataFrames need to be pulled first)\n",
        "try:\n",
        "    sample_pd = X_train_oml.pull().head(100)\n",
        "    nan_count = sum([sample_pd[col].isna().any() for col in feature_cols if col in sample_pd.columns])\n",
        "    inf_count = sum([np.isinf(sample_pd[col]).any() for col in feature_cols if col in sample_pd.columns and pd.api.types.is_numeric_dtype(sample_pd[col])])\n",
        "    print(f\"  Features with NaN (sample): {nan_count}\")\n",
        "    print(f\"  Features with Inf (sample): {inf_count}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Data quality check skipped: {e}\")\n",
        "\n",
        "# Create and Train RandomForest model\n",
        "# NOTE: OML4Py RandomForest uses the SAME API pattern as XGBoost:\n",
        "#   - Both: oml.rf('classification').fit(X, y) or oml.xgb('classification').fit(X, y)\n",
        "# NOTE: OML4Py XGBoost with default parameters achieved AUC ~0.50 (essentially random)\n",
        "# RandomForest achieved AUC 0.9190 in Task 2.8 validation, so we're using it instead\n",
        "print(\"\\nCreating RandomForest model...\")\n",
        "print(\"  ‚úì RandomForest achieved AUC 0.9190 in Task 2.8 validation\")\n",
        "print(\"  ‚úì OML4Py XGBoost defaults gave AUC ~0.50 (random)\")\n",
        "print(\"  ‚ö†Ô∏è  Using default RandomForest parameters (OML4Py doesn't support hyperparameters)\")\n",
        "\n",
        "# OML4Py RandomForest API: Create model, then fit with model_name in .fit()\n",
        "# Based on Oracle docs: oml.rf(**settings).fit(X, y, model_name='MODEL_NAME')\n",
        "print(\"\\nTraining started...\")\n",
        "print(\"  This may take a few minutes...\")\n",
        "print(\"  Training RandomForest in-database...\")\n",
        "\n",
        "model_type = None\n",
        "try:\n",
        "    # Create RandomForest model (no arguments - creates new model)\n",
        "    xgb_model = oml.rf()\n",
        "    print(\"  ‚úì RandomForest model created\")\n",
        "    model_type = \"RandomForest\"\n",
        "    # Train with model_name in .fit() if needed (optional)\n",
        "    xgb_model = xgb_model.fit(X_train_oml, y_train_oml)\n",
        "    print(\"  ‚úì RandomForest training completed\")\n",
        "except Exception as e:\n",
        "    print(f\"  ‚ùå RandomForest failed: {e}\")\n",
        "    print(\"  ‚ö†Ô∏è  Falling back to XGBoost (performance may be poor, AUC ~0.50)\")\n",
        "    # Fallback to XGBoost (even though it performs poorly)\n",
        "    xgb_model = oml.xgb('classification')\n",
        "    model_type = \"XGBoost\"\n",
        "    xgb_model = xgb_model.fit(X_train_oml, y_train_oml)\n",
        "    print(\"  ‚ö†Ô∏è  Using XGBoost as fallback\")\n",
        "\n",
        "print(\"‚úì Training completed!\")\n",
        "print(f\"\\nüìä Model Type: {model_type}\")\n",
        "print(f\"   If this shows 'XGBoost', RandomForest failed and performance will be poor (AUC ~0.50)\")\n",
        "print(f\"   If this shows 'RandomForest', performance should be good (AUC ~0.90)\")\n",
        "\n",
        "# Try to get feature importance (if available)\n",
        "try:\n",
        "    # Some OML4Py versions support feature_importance\n",
        "    if hasattr(xgb_model, 'feature_importance'):\n",
        "        importance = xgb_model.feature_importance()\n",
        "        print(f\"\\n‚úì Feature importance available\")\n",
        "    elif hasattr(xgb_model, 'get_feature_importance'):\n",
        "        importance = xgb_model.get_feature_importance()\n",
        "        print(f\"\\n‚úì Feature importance available\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Feature importance not directly available\")\n",
        "        print(\"   This is normal for some OML4Py versions\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  Could not get feature importance: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Cell 5: Evaluate Model\n",
        "# Prepare test data\n",
        "test_combined_pd = X_test_pd.copy()\n",
        "test_combined_pd['CHURNED'] = y_test_pd.values\n",
        "test_oml = oml.push(test_combined_pd)\n",
        "X_test_oml = test_oml[feature_cols]\n",
        "\n",
        "print(f\"‚úì Test data prepared: {X_test_oml.shape}\")\n",
        "print(f\"‚úì Test churn rate: {y_test_pd.mean()*100:.2f}%\")\n",
        "print(f\"‚úì Expected predictions: {len(y_test_pd)}\")\n",
        "\n",
        "# Get predictions - try without supplemental_cols first\n",
        "print(\"\\nGenerating predictions...\")\n",
        "try:\n",
        "    y_pred_proba_oml = xgb_model.predict_proba(X_test_oml)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Convert to numpy - handle different return formats\n",
        "y_pred_proba_pd = y_pred_proba_oml.pull()\n",
        "print(f\"‚úì Predictions pulled\")\n",
        "print(f\"  Type: {type(y_pred_proba_pd)}\")\n",
        "if hasattr(y_pred_proba_pd, 'shape'):\n",
        "    print(f\"  Shape: {y_pred_proba_pd.shape}\")\n",
        "\n",
        "# Extract probabilities - handle different formats\n",
        "if isinstance(y_pred_proba_pd, pd.DataFrame):\n",
        "    print(f\"  DataFrame columns: {list(y_pred_proba_pd.columns)}\")\n",
        "    if 1 in y_pred_proba_pd.columns:\n",
        "        y_pred_proba = y_pred_proba_pd[1].values\n",
        "    elif len(y_pred_proba_pd.columns) == 2:\n",
        "        y_pred_proba = y_pred_proba_pd.iloc[:, 1].values\n",
        "    else:\n",
        "        y_pred_proba = y_pred_proba_pd.values.flatten()\n",
        "elif isinstance(y_pred_proba_pd, pd.Series):\n",
        "    y_pred_proba = y_pred_proba_pd.values\n",
        "else:\n",
        "    y_pred_proba = np.array(y_pred_proba_pd).flatten()\n",
        "\n",
        "print(f\"  Extracted probabilities shape: {y_pred_proba.shape}\")\n",
        "\n",
        "# Check if probabilities are in 0-100 range and normalize\n",
        "if y_pred_proba.max() > 1.0:\n",
        "    print(\"‚ö†Ô∏è  Probabilities appear to be in 0-100 range, normalizing to 0-1...\")\n",
        "    y_pred_proba = y_pred_proba / 100.0\n",
        "\n",
        "# Ensure probabilities are in valid range\n",
        "y_pred_proba = np.clip(y_pred_proba, 0.0, 1.0)\n",
        "\n",
        "# Get test labels\n",
        "y_test_vals = y_test_pd.values\n",
        "print(f\"\\nShape check:\")\n",
        "print(f\"  Test labels: {len(y_test_vals)}\")\n",
        "print(f\"  Predictions: {len(y_pred_proba)}\")\n",
        "\n",
        "# Handle shape mismatch - take only the first len(y_test_vals) predictions\n",
        "if len(y_pred_proba) != len(y_test_vals):\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: Shape mismatch detected!\")\n",
        "    print(f\"   Predictions: {len(y_pred_proba)}\")\n",
        "    print(f\"   Test labels: {len(y_test_vals)}\")\n",
        "    \n",
        "    if len(y_pred_proba) > len(y_test_vals):\n",
        "        print(f\"   Taking first {len(y_test_vals)} predictions to match test set...\")\n",
        "        y_pred_proba = y_pred_proba[:len(y_test_vals)]\n",
        "    else:\n",
        "        print(f\"   ERROR: Not enough predictions! This shouldn't happen.\")\n",
        "        raise ValueError(f\"Predictions ({len(y_pred_proba)}) < Test labels ({len(y_test_vals)})\")\n",
        "\n",
        "# Check prediction distribution\n",
        "print(f\"\\nPrediction Statistics:\")\n",
        "print(f\"  Mean probability: {y_pred_proba.mean():.4f}\")\n",
        "print(f\"  Min probability: {y_pred_proba.min():.4f}\")\n",
        "print(f\"  Max probability: {y_pred_proba.max():.4f}\")\n",
        "print(f\"  Std probability: {y_pred_proba.std():.4f}\")\n",
        "\n",
        "# If predictions are all the same, model isn't learning\n",
        "if y_pred_proba.std() < 0.01:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Predictions have very low variance!\")\n",
        "    print(\"   This suggests the model is not learning from the data.\")\n",
        "\n",
        "# Create binary predictions\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# Verify shapes match before calculating metrics\n",
        "assert len(y_test_vals) == len(y_pred_proba) == len(y_pred), \\\n",
        "    f\"Shape mismatch: y_test={len(y_test_vals)}, y_pred_proba={len(y_pred_proba)}, y_pred={len(y_pred)}\"\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test_vals, y_pred)\n",
        "precision = precision_score(y_test_vals, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test_vals, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test_vals, y_pred, zero_division=0)\n",
        "auc = roc_auc_score(y_test_vals, y_pred_proba)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model Performance Metrics\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"AUC-ROC:     {auc:.4f} ({auc*100:.2f}%)\")\n",
        "print(f\"Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision:   {precision:.4f} ({precision*100:.2f}%)\")\n",
        "print(f\"Recall:      {recall:.4f} ({recall*100:.2f}%)\")\n",
        "print(f\"F1 Score:    {f1:.4f}\")\n",
        "\n",
        "# Performance assessment\n",
        "if auc < 0.55:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Model performance is very poor (AUC < 0.55)\")\n",
        "    print(\"   This suggests the model is not learning effectively.\")\n",
        "    print(\"   Recommendations:\")\n",
        "    print(\"   1. Check data quality and feature engineering\")\n",
        "    print(\"   2. Verify features are informative\")\n",
        "    print(\"   3. Consider feature selection\")\n",
        "    print(\"   4. Try different hyperparameters\")\n",
        "elif auc < 0.70:\n",
        "    print(\"\\n‚ö†Ô∏è  Model performance is below target (AUC < 0.70)\")\n",
        "    print(\"   Consider improving features or hyperparameter tuning\")\n",
        "else:\n",
        "    print(\"\\n‚úì Model performance is acceptable (AUC >= 0.70)\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_vals, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"                Predicted\")\n",
        "print(\"              Non-Churn  Churn\")\n",
        "print(f\"Actual Non-Churn   {tn:5d}   {fp:5d}\")\n",
        "print(f\"       Churn       {fn:5d}   {tp:5d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Cell 6: Save Model\n",
        "# Save model to OML datastore\n",
        "model_name = 'churn_xgboost_v1'\n",
        "description = f'Churn prediction XGBoost model - AUC: {auc:.4f}'\n",
        "\n",
        "try:\n",
        "    oml.ds.save(\n",
        "        {'model': xgb_model},\n",
        "        model_name,\n",
        "        description=description,\n",
        "        overwrite=True\n",
        "    )\n",
        "    print(f\"‚úì Model saved to OML datastore: {model_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Failed to save model: {e}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e241b3f9",
      "metadata": {},
      "source": [
        "%md\n",
        "\n",
        "# Churn Analysis - Development Notebook\n",
        "## Phase 1: Original Model (Baseline)\n",
        "\n",
        "This notebook develops an XGBoost churn prediction model using OML4Py on Oracle Autonomous Database.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Oracle Autonomous Database with OML enabled\n",
        "- ADMIN and OML schema access\n",
        "- Database objects created (LOGIN_EVENTS table, indexes, grants)\n",
        "\n",
        "**Segment Definition:**\n",
        "- VIP: Has affinity card (AFFINITY_CARD > 0)\n",
        "- Regular: 2+ orders OR $500+ spent\n",
        "- New: Exactly 1 order\n",
        "- Dormant: No orders in 2+ months\n",
        "- At-Risk: Everyone else\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efe60ba",
      "metadata": {
        "oml_type": "script"
      },
      "outputs": [],
      "source": [
        "%script\n",
        "\n",
        "-- Create CHURN_FEATURES view (original version - no SUPPORT_TICKETS table)\n",
        "CREATE OR REPLACE VIEW OML.CHURN_FEATURES AS\n",
        "SELECT \n",
        "    u.ID AS USER_ID,\n",
        "    u.CUST_YEAR_OF_BIRTH,\n",
        "    u.CUST_MARITAL_STATUS,\n",
        "    u.CUST_INCOME_LEVEL,\n",
        "    u.CUST_CREDIT_LIMIT,\n",
        "    u.GENDER,\n",
        "    u.EDUCATION,\n",
        "    u.OCCUPATION,\n",
        "    u.HOUSEHOLD_SIZE,\n",
        "    u.YRS_RESIDENCE,\n",
        "    u.AFFINITY_CARD,\n",
        "    \n",
        "    -- Purchase behavior (24 months)\n",
        "    COALESCE(o_stats.ORDER_COUNT_24M, 0) AS ORDER_COUNT_24M,\n",
        "    COALESCE(o_stats.TOTAL_SPENT_24M, 0) AS TOTAL_SPENT_24M,\n",
        "    COALESCE(o_stats.AVG_ORDER_VALUE_24M, 0) AS AVG_ORDER_VALUE_24M,\n",
        "    COALESCE(o_stats.MONTHS_SINCE_LAST_PURCHASE, 999) AS MONTHS_SINCE_LAST_PURCHASE,\n",
        "    MONTHS_BETWEEN(SYSDATE, u.CREATED_AT) AS CUSTOMER_AGE_MONTHS,\n",
        "    CASE \n",
        "        WHEN COALESCE(o_stats.ORDER_COUNT_24M, 0) > 0 AND COALESCE(o_stats.MONTHS_SINCE_LAST_PURCHASE, 999) > 0 \n",
        "        THEN COALESCE(o_stats.ORDER_COUNT_24M, 0) / NULLIF(o_stats.MONTHS_SINCE_LAST_PURCHASE, 0)\n",
        "        ELSE 0\n",
        "    END AS PURCHASE_VELOCITY,\n",
        "    \n",
        "    -- Login activity (30 days)\n",
        "    COALESCE(login_stats.LOGIN_COUNT_30D, 0) AS LOGIN_COUNT_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(login_stats.LOGIN_COUNT_30D, 0) >= 15 THEN 'High'\n",
        "        WHEN COALESCE(login_stats.LOGIN_COUNT_30D, 0) >= 7 THEN 'Medium'\n",
        "        WHEN COALESCE(login_stats.LOGIN_COUNT_30D, 0) > 0 THEN 'Low'\n",
        "        ELSE 'None'\n",
        "    END AS LOGIN_FREQUENCY_CATEGORY,\n",
        "    COALESCE(login_stats.MONTHS_SINCE_LAST_LOGIN, 999) AS MONTHS_SINCE_LAST_LOGIN,\n",
        "    \n",
        "    -- Support tickets (24 months) - placeholder since table doesn't exist\n",
        "    0 AS SUPPORT_TICKETS_24M,\n",
        "    \n",
        "    -- Review and NPS (24 months)\n",
        "    COALESCE(review_stats.AVG_REVIEW_RATING, 0) AS AVG_REVIEW_RATING,\n",
        "    COALESCE(review_stats.REVIEW_COUNT, 0) AS REVIEW_COUNT,\n",
        "    COALESCE(review_stats.DETRACTOR_COUNT, 0) AS DETRACTOR_COUNT,\n",
        "    COALESCE(review_stats.PASSIVE_COUNT, 0) AS PASSIVE_COUNT,\n",
        "    COALESCE(review_stats.PROMOTER_COUNT, 0) AS PROMOTER_COUNT,\n",
        "    CASE \n",
        "        WHEN COALESCE(review_stats.REVIEW_COUNT, 0) > 0 THEN\n",
        "            ((COALESCE(review_stats.PROMOTER_COUNT, 0) - COALESCE(review_stats.DETRACTOR_COUNT, 0)) / NULLIF(review_stats.REVIEW_COUNT, 0)) * 100\n",
        "        ELSE 0\n",
        "    END AS NPS_SCORE,\n",
        "    CASE WHEN COALESCE(review_stats.NEGATIVE_REVIEWS_90D, 0) > 0 THEN 1 ELSE 0 END AS HAS_NEGATIVE_SENTIMENT,\n",
        "    COALESCE(review_stats.NEGATIVE_REVIEWS_90D, 0) AS NEGATIVE_REVIEWS_90D,\n",
        "    \n",
        "    -- Email engagement (30 days)\n",
        "    COALESCE(email_stats.EMAILS_SENT_30D, 0) AS EMAILS_SENT_30D,\n",
        "    COALESCE(email_stats.EMAILS_OPENED_30D, 0) AS EMAILS_OPENED_30D,\n",
        "    COALESCE(email_stats.EMAILS_CLICKED_30D, 0) AS EMAILS_CLICKED_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(email_stats.EMAILS_SENT_30D, 0) > 0 THEN\n",
        "            COALESCE(email_stats.EMAILS_OPENED_30D, 0) / NULLIF(email_stats.EMAILS_SENT_30D, 0)\n",
        "        ELSE 0\n",
        "    END AS EMAIL_OPEN_RATE_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(email_stats.EMAILS_OPENED_30D, 0) > 0 THEN\n",
        "            COALESCE(email_stats.EMAILS_CLICKED_30D, 0) / NULLIF(email_stats.EMAILS_OPENED_30D, 0)\n",
        "        ELSE 0\n",
        "    END AS EMAIL_CLICK_RATE_30D,\n",
        "    CASE WHEN COALESCE(email_stats.HAS_UNSUBSCRIBED, 0) > 0 THEN 1 ELSE 0 END AS HAS_UNSUBSCRIBED,\n",
        "    \n",
        "    -- Cart events (30 days)\n",
        "    COALESCE(cart_stats.CART_ADDITIONS_30D, 0) AS CART_ADDITIONS_30D,\n",
        "    COALESCE(cart_stats.TOTAL_SESSIONS_30D, 0) AS TOTAL_SESSIONS_30D,\n",
        "    CASE \n",
        "        WHEN COALESCE(cart_stats.TOTAL_SESSIONS_30D, 0) > 0 THEN\n",
        "            COALESCE(cart_stats.CART_ADDITIONS_30D, 0) / NULLIF(cart_stats.TOTAL_SESSIONS_30D, 0)\n",
        "        ELSE 0\n",
        "    END AS BROWSE_TO_CART_RATIO,\n",
        "    COALESCE(cart_stats.CART_ABANDONMENTS_30D, 0) AS CART_ABANDONMENTS_30D,\n",
        "    \n",
        "    -- Returns (24 months)\n",
        "    COALESCE(return_stats.SIZE_FIT_RETURNS_COUNT, 0) AS SIZE_FIT_RETURNS_COUNT,\n",
        "    CASE WHEN COALESCE(return_stats.SIZE_FIT_RETURNS_COUNT, 0) >= 2 THEN 1 ELSE 0 END AS HAS_2PLUS_SIZE_RETURNS,\n",
        "    COALESCE(return_stats.TOTAL_RETURNS_COUNT, 0) AS TOTAL_RETURNS_COUNT,\n",
        "    \n",
        "    -- Customer segment\n",
        "    CASE \n",
        "        WHEN COALESCE(u.AFFINITY_CARD, 0) > 0 THEN 'VIP'\n",
        "        WHEN COALESCE(o_stats.ORDER_COUNT_24M, 0) >= 2 \n",
        "             OR COALESCE(o_stats.TOTAL_SPENT_24M, 0) >= 500 THEN 'Regular'\n",
        "        WHEN COALESCE(o_stats.ORDER_COUNT_24M, 0) = 1 THEN 'New'\n",
        "        WHEN MONTHS_BETWEEN(SYSDATE, COALESCE(o_stats.LAST_PURCHASE_DATE, u.CREATED_AT)) >= 2 THEN 'Dormant'\n",
        "        ELSE 'At-Risk'\n",
        "    END AS CUSTOMER_SEGMENT,\n",
        "    \n",
        "    -- Estimated LTV\n",
        "    COALESCE(o_stats.TOTAL_SPENT_24M, 0) * 2 AS ESTIMATED_LTV\n",
        "    \n",
        "FROM ADMIN.USERS u\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        COUNT(DISTINCT ID) AS ORDER_COUNT_24M,\n",
        "        SUM(TOTAL) AS TOTAL_SPENT_24M,\n",
        "        AVG(TOTAL) AS AVG_ORDER_VALUE_24M,\n",
        "        MAX(CREATED_AT) AS LAST_PURCHASE_DATE,\n",
        "        MONTHS_BETWEEN(SYSDATE, MAX(CREATED_AT)) AS MONTHS_SINCE_LAST_PURCHASE\n",
        "    FROM ADMIN.ORDERS\n",
        "    WHERE STATUS NOT IN ('cancelled')\n",
        "      AND CREATED_AT >= ADD_MONTHS(SYSDATE, -24)\n",
        "    GROUP BY USER_ID\n",
        ") o_stats ON u.ID = o_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        COUNT(*) AS LOGIN_COUNT_30D,\n",
        "        MONTHS_BETWEEN(SYSDATE, MAX(LOGIN_TIMESTAMP)) AS MONTHS_SINCE_LAST_LOGIN\n",
        "    FROM ADMIN.LOGIN_EVENTS\n",
        "    WHERE LOGIN_TIMESTAMP >= SYSDATE - 30\n",
        "    GROUP BY USER_ID\n",
        ") login_stats ON u.ID = login_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        AVG(RATING) AS AVG_REVIEW_RATING,\n",
        "        COUNT(*) AS REVIEW_COUNT,\n",
        "        SUM(CASE WHEN RATING <= 2 THEN 1 ELSE 0 END) AS DETRACTOR_COUNT,\n",
        "        SUM(CASE WHEN RATING = 3 THEN 1 ELSE 0 END) AS PASSIVE_COUNT,\n",
        "        SUM(CASE WHEN RATING >= 4 THEN 1 ELSE 0 END) AS PROMOTER_COUNT,\n",
        "        SUM(CASE WHEN RATING <= 2 AND CREATED_AT >= SYSDATE - 90 THEN 1 ELSE 0 END) AS NEGATIVE_REVIEWS_90D\n",
        "    FROM ADMIN.PRODUCT_REVIEWS\n",
        "    WHERE CREATED_AT >= ADD_MONTHS(SYSDATE, -24)\n",
        "    GROUP BY USER_ID\n",
        ") review_stats ON u.ID = review_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        COUNT(*) AS EMAILS_SENT_30D,\n",
        "        SUM(CASE WHEN OPENED_AT IS NOT NULL THEN 1 ELSE 0 END) AS EMAILS_OPENED_30D,\n",
        "        SUM(CASE WHEN CLICKED_AT IS NOT NULL THEN 1 ELSE 0 END) AS EMAILS_CLICKED_30D,\n",
        "        SUM(CASE WHEN UNSUBSCRIBED_AT IS NOT NULL THEN 1 ELSE 0 END) AS HAS_UNSUBSCRIBED\n",
        "    FROM ADMIN.EMAIL_ENGAGEMENT\n",
        "    WHERE SENT_AT >= SYSDATE - 30\n",
        "    GROUP BY USER_ID\n",
        ") email_stats ON u.ID = email_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        SUM(CASE WHEN ACTION = 'added' THEN 1 ELSE 0 END) AS CART_ADDITIONS_30D,\n",
        "        COUNT(DISTINCT SESSION_ID) AS TOTAL_SESSIONS_30D,\n",
        "        SUM(CASE WHEN ACTION = 'abandoned' THEN 1 ELSE 0 END) AS CART_ABANDONMENTS_30D\n",
        "    FROM ADMIN.CART_EVENTS\n",
        "    WHERE CREATED_AT >= SYSDATE - 30\n",
        "    GROUP BY USER_ID\n",
        ") cart_stats ON u.ID = cart_stats.USER_ID\n",
        "LEFT JOIN (\n",
        "    SELECT \n",
        "        USER_ID,\n",
        "        SUM(CASE WHEN RETURN_REASON IN ('SIZE_TOO_SMALL', 'SIZE_TOO_LARGE') THEN 1 ELSE 0 END) AS SIZE_FIT_RETURNS_COUNT,\n",
        "        COUNT(*) AS TOTAL_RETURNS_COUNT\n",
        "    FROM ADMIN.RETURNS\n",
        "    WHERE REQUESTED_AT >= ADD_MONTHS(SYSDATE, -24)\n",
        "    GROUP BY USER_ID\n",
        ") return_stats ON u.ID = return_stats.USER_ID\n",
        "WHERE u.IS_ACTIVE = 1;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a72352f1",
      "metadata": {
        "oml_type": "script"
      },
      "outputs": [],
      "source": [
        "%script\n",
        "\n",
        "-- Create CHURN_TRAINING_DATA view (original version with adjusted churn definition)\n",
        "CREATE OR REPLACE VIEW OML.CHURN_TRAINING_DATA AS\n",
        "SELECT \n",
        "    cf.*,\n",
        "    CASE \n",
        "        WHEN cf.CUSTOMER_SEGMENT = 'Dormant' THEN 1\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE >= 0.75 THEN 1\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE >= 0.5\n",
        "             AND (\n",
        "                 cf.LOGIN_COUNT_30D <= 7\n",
        "                 OR cf.EMAIL_OPEN_RATE_30D < 0.5\n",
        "                 OR cf.CART_ABANDONMENTS_30D >= 2\n",
        "             ) THEN 1\n",
        "        WHEN cf.LOGIN_COUNT_30D <= 3 THEN 1\n",
        "        ELSE 0\n",
        "    END AS CHURNED_60_90D\n",
        "FROM OML.CHURN_FEATURES cf\n",
        "WHERE cf.CUSTOMER_AGE_MONTHS >= 3\n",
        "    AND cf.ORDER_COUNT_24M > 0;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f48acf2",
      "metadata": {
        "oml_type": "script"
      },
      "outputs": [],
      "source": [
        "%script\n",
        "\n",
        "-- Verify views created\n",
        "SELECT COUNT(*) AS FEATURES_COUNT FROM OML.CHURN_FEATURES;\n",
        "SELECT COUNT(*) AS TRAINING_COUNT FROM OML.CHURN_TRAINING_DATA;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f72612",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "import oml\n",
        "\n",
        "# Verify connection\n",
        "print(\"OML Connected:\", oml.isconnected())\n",
        "\n",
        "# Check OML version\n",
        "try:\n",
        "    print(\"OML Version:\", oml.__version__)\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be42d30",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Load features\n",
        "features = oml.sync(view='CHURN_FEATURES')\n",
        "\n",
        "# Basic info\n",
        "print(\"=\" * 60)\n",
        "print(\"CHURN_FEATURES Data Overview\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Shape:\", features.shape)\n",
        "print(\"\\nColumn count:\", len(features.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bea7d51d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Check customer segments distribution\n",
        "import pandas as pd\n",
        "\n",
        "if 'CUSTOMER_SEGMENT' in features.columns:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Customer Segment Distribution\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    features_pd = features.pull()\n",
        "    segment_dist = features_pd['CUSTOMER_SEGMENT'].value_counts()\n",
        "    print(segment_dist)\n",
        "    \n",
        "    print(\"\\nPercentage distribution:\")\n",
        "    print((segment_dist / len(features_pd) * 100).round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14576dde",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Check the churn distribution\n",
        "import pandas as pd\n",
        "\n",
        "train_data = oml.sync(view='CHURN_TRAINING_DATA')\n",
        "train_data_pd = train_data.pull()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Adjusted Churn Definition Results\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Training Data Shape: \" + str(train_data_pd.shape))\n",
        "print(\"\\nTarget Variable Distribution:\")\n",
        "print(train_data_pd['CHURNED_60_90D'].value_counts())\n",
        "churn_rate = train_data_pd['CHURNED_60_90D'].mean() * 100\n",
        "print(\"\\nChurn Rate: \" + str(churn_rate) + \" %\")\n",
        "print(\"Non-Churn Rate: \" + str(100 - churn_rate) + \" %\")\n",
        "print(\"\\n✓ Good churn rate for training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8136f1ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Prepare features and split data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load training data\n",
        "train_data = oml.sync(view='CHURN_TRAINING_DATA')\n",
        "train_data_pd = train_data.pull()\n",
        "\n",
        "# Identify feature columns (exclude target and metadata)\n",
        "exclude_cols = ['USER_ID', 'CUSTOMER_SEGMENT', 'ESTIMATED_LTV', 'CHURNED_60_90D']\n",
        "feature_cols = [col for col in train_data_pd.columns if col not in exclude_cols]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Preparing Features and Splitting Data\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Total features: \" + str(len(feature_cols)))\n",
        "print(\"Features: \" + \", \".join(feature_cols[:10]) + \"...\")\n",
        "\n",
        "# Prepare X and y\n",
        "X_pd = train_data_pd[feature_cols].copy()\n",
        "y_pd = train_data_pd['CHURNED_60_90D']\n",
        "\n",
        "# Clean data - replace NaN and infinity\n",
        "for col in feature_cols:\n",
        "    if pd.api.types.is_numeric_dtype(X_pd[col]):\n",
        "        X_pd[col] = X_pd[col].replace([np.inf, -np.inf], np.nan)\n",
        "        X_pd[col] = X_pd[col].fillna(0)\n",
        "\n",
        "# Stratified split\n",
        "X_train_pd, X_test_pd, y_train_pd, y_test_pd = train_test_split(\n",
        "    X_pd, y_pd, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_pd\n",
        ")\n",
        "\n",
        "print(\"\\nSplit completed:\")\n",
        "print(\"  Train size: \" + str(len(X_train_pd)))\n",
        "print(\"  Test size: \" + str(len(X_test_pd)))\n",
        "print(\"  Train churn rate: \" + str(round(y_train_pd.mean() * 100, 2)) + \"%\")\n",
        "print(\"  Test churn rate: \" + str(round(y_test_pd.mean() * 100, 2)) + \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700dbd4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Train XGBoost - Simplified approach\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training XGBoost Model\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Merge X_train and y_train for database push\n",
        "train_combined_pd = X_train_pd.copy()\n",
        "train_combined_pd['CHURNED_60_90D'] = y_train_pd.values\n",
        "\n",
        "# Push to database\n",
        "print(\"Pushing training data to database...\")\n",
        "train_oml = oml.push(train_combined_pd)\n",
        "print(\"Training data pushed: \" + str(train_oml.shape))\n",
        "\n",
        "# Create XGBoost model with explicit classification type\n",
        "xgb_model = oml.xgb('classification')\n",
        "\n",
        "# Get features and target from OML DataFrame\n",
        "X_train_oml = train_oml[feature_cols]\n",
        "y_train_oml = train_oml['CHURNED_60_90D']\n",
        "\n",
        "print(\"X_train_oml shape: \" + str(X_train_oml.shape))\n",
        "print(\"Training started...\")\n",
        "\n",
        "# Fit the model\n",
        "xgb_model = xgb_model.fit(X_train_oml, y_train_oml)\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb74935",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Evaluate model performance\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Prepare test data in OML format\n",
        "test_combined_pd = X_test_pd.copy()\n",
        "test_combined_pd['CHURNED_60_90D'] = y_test_pd.values\n",
        "test_oml = oml.push(test_combined_pd)\n",
        "X_test_oml = test_oml[feature_cols]\n",
        "\n",
        "# Get predictions - OML returns Vector objects\n",
        "print(\"Generating predictions...\")\n",
        "y_pred_proba_oml = xgb_model.predict_proba(X_test_oml)\n",
        "\n",
        "# Convert OML Vector to numpy array\n",
        "y_pred_proba_pd = y_pred_proba_oml.pull()\n",
        "if isinstance(y_pred_proba_pd, pd.DataFrame):\n",
        "    if 1 in y_pred_proba_pd.columns:\n",
        "        y_pred_proba = y_pred_proba_pd[1].values\n",
        "    elif len(y_pred_proba_pd.columns) == 2:\n",
        "        y_pred_proba = y_pred_proba_pd.iloc[:, 1].values\n",
        "    else:\n",
        "        y_pred_proba = y_pred_proba_pd.values.flatten()\n",
        "else:\n",
        "    y_pred_proba = np.array(y_pred_proba_pd)\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# Convert y_test to numpy array for metrics\n",
        "y_test_vals = y_test_pd.values\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test_vals, y_pred)\n",
        "precision = precision_score(y_test_vals, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test_vals, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test_vals, y_pred, zero_division=0)\n",
        "auc = roc_auc_score(y_test_vals, y_pred_proba)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_vals, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Accuracy:  \" + str(round(accuracy, 4)))\n",
        "print(\"Precision: \" + str(round(precision, 4)) + \"  (of predicted churn, how many actually churn)\")\n",
        "print(\"Recall:    \" + str(round(recall, 4)) + \"  (of actual churn, how many did we catch)\")\n",
        "print(\"F1 Score:  \" + str(round(f1, 4)) + \"  (harmonic mean of precision and recall)\")\n",
        "print(\"AUC-ROC:   \" + str(round(auc, 4)) + \"  (model's ability to distinguish classes)\")\n",
        "print(\"\\nModel Confidence: \" + str(int(auc * 100)) + \"%\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"                Predicted\")\n",
        "print(\"              Non-Churn  Churn\")\n",
        "print(\"Actual Non-Churn   \" + str(tn) + \"   \" + str(fp))\n",
        "print(\"       Churn       \" + str(fn) + \"   \" + str(tp))\n",
        "print(\"\\nTrue Negatives:  \" + str(tn) + \" (correctly predicted non-churn)\")\n",
        "print(\"False Positives: \" + str(fp) + \" (predicted churn, but didn't churn)\")\n",
        "print(\"False Negatives: \" + str(fn) + \" (missed churners)\")\n",
        "print(\"True Positives:  \" + str(tp) + \" (correctly predicted churn)\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_vals, y_pred, target_names=['Non-Churn', 'Churn']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6fa73c",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Get and display feature importance - OML-compatible\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Feature Importance Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get importance from model (it's a property, not a method!)\n",
        "try:\n",
        "    print(\"\\nExtracting feature importance from xgb_model.importance...\")\n",
        "    importance_result = xgb_model.importance  # Property, not method\n",
        "    \n",
        "    # Pull OML DataFrame to pandas\n",
        "    if hasattr(importance_result, 'pull'):\n",
        "        importance_df_raw = importance_result.pull()\n",
        "    else:\n",
        "        importance_df_raw = importance_result\n",
        "    \n",
        "    # OML XGBoost importance has columns: ATTRIBUTE_NAME and GAIN\n",
        "    if 'ATTRIBUTE_NAME' in importance_df_raw.columns and 'GAIN' in importance_df_raw.columns:\n",
        "        feature_importance = importance_df_raw[['ATTRIBUTE_NAME', 'GAIN']].copy()\n",
        "        feature_importance.columns = ['FEATURE_NAME', 'IMPORTANCE_SCORE']\n",
        "        importance_method = \"xgb_model.importance - GAIN metric\"\n",
        "        \n",
        "        # Sort by importance\n",
        "        importance_df = feature_importance.sort_values('IMPORTANCE_SCORE', ascending=False)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Top 20 Most Important Features (by GAIN)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(importance_df.head(20).to_string(index=False))\n",
        "        \n",
        "        # Analyze feature importance distribution\n",
        "        print(\"\\nTotal features with importance data: \" + str(len(importance_df)))\n",
        "        positive_importance = importance_df[importance_df['IMPORTANCE_SCORE'] > 0]\n",
        "        print(\"Features with positive importance: \" + str(len(positive_importance)))\n",
        "        \n",
        "        if len(positive_importance) > 0:\n",
        "            total_importance = importance_df['IMPORTANCE_SCORE'].sum()\n",
        "            if abs(total_importance) > 1e-10:\n",
        "                top10_pct = importance_df.head(10)['IMPORTANCE_SCORE'].sum() / total_importance * 100\n",
        "                top5_pct = importance_df.head(5)['IMPORTANCE_SCORE'].sum() / total_importance * 100\n",
        "                print(\"Top 5 features account for: \" + str(round(top5_pct, 1)) + \"% of total importance\")\n",
        "                print(\"Top 10 features account for: \" + str(round(top10_pct, 1)) + \"% of total importance\")\n",
        "        \n",
        "        # Store for later use\n",
        "        importance_df_result = importance_df\n",
        "    else:\n",
        "        print(\"Could not find expected columns in importance data\")\n",
        "        importance_df_result = None\n",
        "        \n",
        "except Exception as e:\n",
        "    print(\"Failed to extract importance: \" + str(e))\n",
        "    importance_df_result = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442ff25b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Save model to OML datastore\n",
        "model_name = 'CHURN_XGBOOST_MODEL'\n",
        "\n",
        "# Save - correct syntax: first arg is dict of objects, then name\n",
        "oml.ds.save({'churn_xgb_model': xgb_model}, model_name, description='Churn XGBoost Model v1', overwrite=True)\n",
        "print(\"✓ Model '\" + model_name + \"' saved to OML datastore\")\n",
        "\n",
        "# Verify model is persisted\n",
        "print(\"✓ Model is persisted in Oracle Database\")\n",
        "print(\"Model type: \" + str(type(xgb_model)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b2b951",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Find optimal probability threshold\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Use predictions from Step 6 (y_test_vals and y_pred_proba should be available)\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "f1_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "accuracies = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test_vals, y_pred_thresh, zero_division=0)\n",
        "    prec = precision_score(y_test_vals, y_pred_thresh, zero_division=0)\n",
        "    rec = recall_score(y_test_vals, y_pred_thresh, zero_division=0)\n",
        "    acc = accuracy_score(y_test_vals, y_pred_thresh)\n",
        "    f1_scores.append(f1)\n",
        "    precisions.append(prec)\n",
        "    recalls.append(rec)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Find optimal threshold (maximize F1)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"THRESHOLD OPTIMIZATION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimal Threshold: \" + str(round(optimal_threshold, 3)) + \" (instead of 0.5)\")\n",
        "print(\"F1 Score at optimal: \" + str(round(f1_scores[optimal_idx], 4)))\n",
        "print(\"Precision at optimal: \" + str(round(precisions[optimal_idx], 4)))\n",
        "print(\"Recall at optimal: \" + str(round(recalls[optimal_idx], 4)))\n",
        "print(\"Accuracy at optimal: \" + str(round(accuracies[optimal_idx], 4)))\n",
        "\n",
        "# Re-evaluate with optimal threshold\n",
        "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "accuracy_opt = accuracy_score(y_test_vals, y_pred_optimal)\n",
        "precision_opt = precision_score(y_test_vals, y_pred_optimal, zero_division=0)\n",
        "recall_opt = recall_score(y_test_vals, y_pred_optimal, zero_division=0)\n",
        "f1_opt = f1_score(y_test_vals, y_pred_optimal, zero_division=0)\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(\"  Threshold 0.5:  F1=\" + str(round(f1, 4)) + \", Precision=\" + str(round(precision, 4)) + \", Recall=\" + str(round(recall, 4)))\n",
        "print(\"  Threshold \" + str(round(optimal_threshold, 3)) + \": F1=\" + str(round(f1_opt, 4)) + \", Precision=\" + str(round(precision_opt, 4)) + \", Recall=\" + str(round(recall_opt, 4)))\n",
        "\n",
        "# Store optimal threshold for later use\n",
        "optimal_threshold_value = optimal_threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a8760a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Score all active customers for churn risk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load current features (all active customers)\n",
        "current_features = oml.sync(view='CHURN_FEATURES')\n",
        "current_features_pd = current_features.pull()\n",
        "\n",
        "# Prepare features\n",
        "X_current_pd = current_features_pd[feature_cols].copy()\n",
        "\n",
        "# Clean data before pushing\n",
        "print(\"Cleaning data before scoring...\")\n",
        "X_current_pd = X_current_pd.replace([np.inf, -np.inf], np.nan)\n",
        "numeric_cols = X_current_pd.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    X_current_pd[col] = pd.to_numeric(X_current_pd[col], errors='coerce').fillna(0)\n",
        "\n",
        "# Push to OML for scoring\n",
        "X_current_oml = oml.push(X_current_pd)\n",
        "\n",
        "# Predict churn probability\n",
        "print(\"=\" * 60)\n",
        "print(\"Scoring All Customers\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Scoring \" + str(len(X_current_pd)) + \" customers...\")\n",
        "churn_proba_oml = xgb_model.predict_proba(X_current_oml)\n",
        "\n",
        "# Convert to numpy array\n",
        "churn_proba_pd = churn_proba_oml.pull()\n",
        "if isinstance(churn_proba_pd, pd.DataFrame):\n",
        "    if 1 in churn_proba_pd.columns:\n",
        "        churn_proba = churn_proba_pd[1].values\n",
        "    elif len(churn_proba_pd.columns) == 2:\n",
        "        churn_proba = churn_proba_pd.iloc[:, 1].values\n",
        "    else:\n",
        "        churn_proba = churn_proba_pd.values.flatten()\n",
        "else:\n",
        "    churn_proba = np.array(churn_proba_pd)\n",
        "\n",
        "# Use optimal threshold (from Step 9) or default to 0.3 if not available\n",
        "threshold = optimal_threshold_value if 'optimal_threshold_value' in globals() else 0.3\n",
        "\n",
        "# Create results DataFrame\n",
        "results = pd.DataFrame({\n",
        "    'USER_ID': current_features_pd['USER_ID'],\n",
        "    'CHURN_RISK_SCORE': churn_proba,\n",
        "    'CHURN_RISK_PERCENT': (churn_proba * 100).round(2),\n",
        "    'IS_AT_RISK': (churn_proba >= threshold).astype(int),\n",
        "    'CUSTOMER_SEGMENT': current_features_pd['CUSTOMER_SEGMENT'],\n",
        "    'ESTIMATED_LTV': current_features_pd['ESTIMATED_LTV']\n",
        "})\n",
        "\n",
        "results['LTV_AT_RISK'] = (results['CHURN_RISK_PERCENT'] / 100 * results['ESTIMATED_LTV']).round(2)\n",
        "\n",
        "# Calculate summary statistics\n",
        "total_customers = len(results)\n",
        "at_risk_count = int(results['IS_AT_RISK'].sum())\n",
        "avg_risk = float(results['CHURN_RISK_PERCENT'].mean())\n",
        "median_risk = float(results['CHURN_RISK_PERCENT'].median())\n",
        "total_ltv_at_risk = float(results['LTV_AT_RISK'].sum())\n",
        "\n",
        "# Risk distribution counts\n",
        "low_risk_count = int((results['CHURN_RISK_PERCENT'] < 30).sum())\n",
        "medium_risk_count = int(((results['CHURN_RISK_PERCENT'] >= 30) & (results['CHURN_RISK_PERCENT'] < 50)).sum())\n",
        "high_risk_count = int(((results['CHURN_RISK_PERCENT'] >= 50) & (results['CHURN_RISK_PERCENT'] < 70)).sum())\n",
        "very_high_risk_count = int((results['CHURN_RISK_PERCENT'] >= 70).sum())\n",
        "\n",
        "# Print results\n",
        "print(\"\\nScoring Results (Threshold: \" + str(round(threshold, 3)) + \"):\")\n",
        "print(\"Total customers scored: \" + str(total_customers))\n",
        "print(\"At-risk customers (>=\" + str(int(threshold*100)) + \"% risk): \" + str(at_risk_count))\n",
        "print(\"Average risk score: \" + str(round(avg_risk, 2)) + \"%\")\n",
        "print(\"Median risk score: \" + str(round(median_risk, 2)) + \"%\")\n",
        "print(\"Total LTV at risk: $\" + format(total_ltv_at_risk, ',.2f'))\n",
        "\n",
        "print(\"\\nRisk Score Distribution:\")\n",
        "print(\"  Low risk (<30%): \" + str(low_risk_count))\n",
        "print(\"  Medium risk (30-50%): \" + str(medium_risk_count))\n",
        "print(\"  High risk (50-70%): \" + str(high_risk_count))\n",
        "print(\"  Very high risk (>=70%): \" + str(very_high_risk_count))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352330e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Analyze metrics by customer segment (cohort)\n",
        "import pandas as pd\n",
        "\n",
        "# Cohort analysis using pandas groupby\n",
        "cohort_metrics = results.groupby('CUSTOMER_SEGMENT').agg({\n",
        "    'USER_ID': 'count',\n",
        "    'CHURN_RISK_PERCENT': 'mean',\n",
        "    'ESTIMATED_LTV': 'sum',\n",
        "    'LTV_AT_RISK': 'sum',\n",
        "    'IS_AT_RISK': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "cohort_metrics.columns = [\n",
        "    'COHORT_NAME', \n",
        "    'CUSTOMER_COUNT', \n",
        "    'AVG_RISK_SCORE', \n",
        "    'TOTAL_LTV', \n",
        "    'LTV_AT_RISK', \n",
        "    'AT_RISK_COUNT'\n",
        "]\n",
        "\n",
        "# Calculate additional metrics\n",
        "cohort_metrics['AT_RISK_PERCENT'] = (cohort_metrics['AT_RISK_COUNT'] / cohort_metrics['CUSTOMER_COUNT'] * 100).round(2)\n",
        "cohort_metrics['LTV_RISK_PERCENT'] = (cohort_metrics['LTV_AT_RISK'] / cohort_metrics['TOTAL_LTV'] * 100).round(2)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Cohort-Level Metrics\")\n",
        "print(\"=\" * 60)\n",
        "print(cohort_metrics.to_string(index=False))\n",
        "\n",
        "# Display summary insights\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Key Insights by Cohort\")\n",
        "print(\"=\" * 60)\n",
        "for _, row in cohort_metrics.iterrows():\n",
        "    print(\"\\n\" + str(row['COHORT_NAME']) + \":\")\n",
        "    print(\"  - \" + str(row['CUSTOMER_COUNT']) + \" customers\")\n",
        "    print(\"  - Avg Risk Score: \" + str(round(row['AVG_RISK_SCORE'], 1)) + \"%\")\n",
        "    print(\"  - At-Risk Count: \" + str(row['AT_RISK_COUNT']) + \" (\" + str(row['AT_RISK_PERCENT']) + \"%)\")\n",
        "    print(\"  - LTV at Risk: $\" + format(row['LTV_AT_RISK'], ',.0f') + \" (\" + str(row['LTV_RISK_PERCENT']) + \"% of total LTV)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8e5e6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Analyze top risk factors driving churn predictions\n",
        "import pandas as pd\n",
        "\n",
        "# Merge results with features\n",
        "merged = results.merge(current_features_pd, on='USER_ID', how='inner', suffixes=('', '_y'))\n",
        "\n",
        "# Check if CUSTOMER_SEGMENT exists, if not get it from results\n",
        "if 'CUSTOMER_SEGMENT' not in merged.columns:\n",
        "    if 'CUSTOMER_SEGMENT' in results.columns:\n",
        "        merged['CUSTOMER_SEGMENT'] = results['CUSTOMER_SEGMENT']\n",
        "    elif 'CUSTOMER_SEGMENT_y' in merged.columns:\n",
        "        merged['CUSTOMER_SEGMENT'] = merged['CUSTOMER_SEGMENT_y']\n",
        "\n",
        "# Analyze each risk factor based on top important features\n",
        "risk_factors = []\n",
        "\n",
        "# 1. Months Since Last Purchase (Top feature - 77% importance)\n",
        "high_months = merged[merged['MONTHS_SINCE_LAST_PURCHASE'] >= 1.5]\n",
        "if len(high_months) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in high_months.columns and len(high_months['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = high_months['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'No Purchase in 45+ Days (1.5+ months)',\n",
        "        'IMPACT_SCORE': str(int(high_months['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(high_months),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(high_months['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': '77%'\n",
        "    })\n",
        "\n",
        "# 2. Cart Abandonments (11.9% importance)\n",
        "high_abandon = merged[merged['CART_ABANDONMENTS_30D'] >= 3]\n",
        "if len(high_abandon) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in high_abandon.columns and len(high_abandon['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = high_abandon['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'High Cart Abandonments (3+)',\n",
        "        'IMPACT_SCORE': str(int(high_abandon['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(high_abandon),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(high_abandon['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': '11.9%'\n",
        "    })\n",
        "\n",
        "# 3. Low Login Activity (10.8% importance)\n",
        "low_login = merged[merged['LOGIN_COUNT_30D'] <= 3]\n",
        "if len(low_login) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in low_login.columns and len(low_login['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = low_login['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'Low Login Activity (≤3 logins)',\n",
        "        'IMPACT_SCORE': str(int(low_login['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(low_login),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(low_login['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': '10.8%'\n",
        "    })\n",
        "\n",
        "# 4. Email Engagement\n",
        "low_email = merged[merged['EMAIL_OPEN_RATE_30D'] < 0.2]\n",
        "if len(low_email) > 0:\n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'Email Engagement Decay',\n",
        "        'IMPACT_SCORE': str(int(low_email['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(low_email),\n",
        "        'PRIMARY_SEGMENT': 'All segments',\n",
        "        'AVG_RISK': float(low_email['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': 'Low'\n",
        "    })\n",
        "\n",
        "# 5. Size/Fit Returns\n",
        "size_returns = merged[merged['HAS_2PLUS_SIZE_RETURNS'] == 1]\n",
        "if len(size_returns) > 0:\n",
        "    primary_seg = 'All'\n",
        "    if 'CUSTOMER_SEGMENT' in size_returns.columns and len(size_returns['CUSTOMER_SEGMENT'].mode()) > 0:\n",
        "        primary_seg = size_returns['CUSTOMER_SEGMENT'].mode().iloc[0]\n",
        "    \n",
        "    risk_factors.append({\n",
        "        'RISK_FACTOR': 'Size/Fit Issues (2+ returns)',\n",
        "        'IMPACT_SCORE': str(int(size_returns['CHURN_RISK_PERCENT'].mean())) + '%',\n",
        "        'AFFECTED_CUSTOMERS': len(size_returns),\n",
        "        'PRIMARY_SEGMENT': primary_seg,\n",
        "        'AVG_RISK': float(size_returns['CHURN_RISK_PERCENT'].mean()),\n",
        "        'FEATURE_IMPORTANCE': 'Low'\n",
        "    })\n",
        "\n",
        "# Create DataFrame and sort by average risk\n",
        "risk_factors_df = pd.DataFrame(risk_factors)\n",
        "if len(risk_factors_df) > 0:\n",
        "    risk_factors_df = risk_factors_df.sort_values('AVG_RISK', ascending=False)\n",
        "    risk_factors_display = risk_factors_df[['RISK_FACTOR', 'IMPACT_SCORE', 'AFFECTED_CUSTOMERS', 'PRIMARY_SEGMENT', 'FEATURE_IMPORTANCE']]\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Top Risk Factors (Based on Model's Important Features)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(risk_factors_display.to_string(index=False))\n",
        "else:\n",
        "    print(\"No risk factors identified\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c15096",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Generate comprehensive summary report\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHURN MODEL DEVELOPMENT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nModel Name: CHURN_XGBOOST_MODEL\")\n",
        "print(\"Model Type: XGBoost Binary Classification\")\n",
        "print(\"Churn Definition: Multi-factor (Dormant, 0.75+ months, activity decline, low login)\")\n",
        "\n",
        "print(\"\\n--- Training Data ---\")\n",
        "print(\"Training Samples: \" + str(len(X_train_pd)))\n",
        "print(\"Test Samples: \" + str(len(X_test_pd)))\n",
        "print(\"Total Features: \" + str(len(feature_cols)))\n",
        "if 'importance_df_result' in globals():\n",
        "    print(\"Features Used by Model: \" + str(len(importance_df_result)))\n",
        "else:\n",
        "    print(\"Features Used by Model: N/A\")\n",
        "\n",
        "print(\"\\n--- Model Performance (Threshold 0.5) ---\")\n",
        "print(\"  Accuracy:  \" + str(round(accuracy, 4)) + \" (\" + str(round(accuracy*100, 2)) + \"%)\")\n",
        "print(\"  Precision: \" + str(round(precision, 4)) + \" (\" + str(round(precision*100, 2)) + \"%)\")\n",
        "print(\"  Recall:    \" + str(round(recall, 4)) + \" (\" + str(round(recall*100, 2)) + \"%)\")\n",
        "print(\"  F1 Score:  \" + str(round(f1, 4)))\n",
        "print(\"  AUC-ROC:   \" + str(round(auc, 4)))\n",
        "print(\"  Model Confidence: \" + str(int(auc * 100)) + \"%\")\n",
        "\n",
        "if 'optimal_threshold_value' in globals():\n",
        "    print(\"\\n--- Model Performance (Optimal Threshold \" + str(round(optimal_threshold_value, 3)) + \") ---\")\n",
        "    print(\"  Accuracy:  \" + str(round(accuracy_opt, 4)) + \" (\" + str(round(accuracy_opt*100, 2)) + \"%)\")\n",
        "    print(\"  Precision: \" + str(round(precision_opt, 4)) + \" (\" + str(round(precision_opt*100, 2)) + \"%)\")\n",
        "    print(\"  Recall:    \" + str(round(recall_opt, 4)) + \" (\" + str(round(recall_opt*100, 2)) + \"%)\")\n",
        "    print(\"  F1 Score:  \" + str(round(f1_opt, 4)))\n",
        "\n",
        "print(\"\\n--- Top 5 Most Important Features ---\")\n",
        "if 'importance_df_result' in globals():\n",
        "    top5 = importance_df_result.head(5)\n",
        "    total_importance = importance_df_result['IMPORTANCE_SCORE'].sum()\n",
        "    for idx, row in top5.iterrows():\n",
        "        pct = (row['IMPORTANCE_SCORE'] / total_importance * 100) if total_importance > 0 else 0\n",
        "        print(\"  \" + str(row['FEATURE_NAME']) + \": \" + str(round(row['IMPORTANCE_SCORE'], 4)) + \" (\" + str(round(pct, 1)) + \"%)\")\n",
        "\n",
        "print(\"\\n--- Scoring Results ---\")\n",
        "threshold = optimal_threshold_value if 'optimal_threshold_value' in globals() else 0.3\n",
        "print(\"  Total Customers Scored: \" + str(len(results)))\n",
        "print(\"  At-Risk Customers (Threshold: \" + str(round(threshold, 3)) + \"): \" + str(int(results['IS_AT_RISK'].sum())))\n",
        "print(\"  Average Risk Score: \" + str(round(results['CHURN_RISK_PERCENT'].mean(), 2)) + \"%\")\n",
        "print(\"  Total LTV at Risk: $\" + format(results['LTV_AT_RISK'].sum(), ',.2f'))\n",
        "\n",
        "print(\"\\n--- Cohort Summary ---\")\n",
        "for _, row in cohort_metrics.iterrows():\n",
        "    print(\"  \" + str(row['COHORT_NAME']) + \": \" + str(row['AT_RISK_COUNT']) + \" at-risk (\" + str(row['AT_RISK_PERCENT']) + \"%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL PERFORMANCE NOTES\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nCurrent Issues:\")\n",
        "print(\"  1. AUC-ROC is low (52%) - barely better than random\")\n",
        "print(\"  2. Precision and Recall are both low (18-20%)\")\n",
        "print(\"  3. Model heavily relies on 3 features (99.9% of importance)\")\n",
        "\n",
        "print(\"\\nRecommended Next Steps:\")\n",
        "print(\"  1. Feature Engineering:\")\n",
        "print(\"     - Model only using 10 features - check if others are being filtered\")\n",
        "print(\"     - Create interaction features (e.g., months_since_purchase * login_count)\")\n",
        "print(\"  2. Hyperparameter Tuning:\")\n",
        "print(\"     - Adjust max_depth, learning_rate, n_estimators\")\n",
        "print(\"     - Try different scale_pos_weight values\")\n",
        "print(\"  3. Churn Definition:\")\n",
        "print(\"     - Current definition may not align with actual churn behavior\")\n",
        "print(\"     - Consider validating churn definition with business stakeholders\")\n",
        "print(\"  4. Data Quality:\")\n",
        "print(\"     - Check if feature distributions make sense\")\n",
        "print(\"     - Verify data quality in top 3 important features\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model development and evaluation complete!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c565b1c4",
      "metadata": {},
      "source": [
        "%md\n",
        "\n",
        "# Phase 2: Enhanced Model with Feature Engineering\n",
        "\n",
        "This section builds upon the original model by adding 17 new engineered features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41844e82",
      "metadata": {
        "oml_type": "script"
      },
      "outputs": [],
      "source": [
        "%script\n",
        "\n",
        "-- Create enhanced features view with interaction and composite features\n",
        "CREATE OR REPLACE VIEW OML.CHURN_FEATURES_ENHANCED AS\n",
        "SELECT \n",
        "    cf.*,\n",
        "    \n",
        "    -- ============================================\n",
        "    -- INTERACTION FEATURES (Top 3 features)\n",
        "    -- ============================================\n",
        "    -- Months since purchase * Login count (captures inactivity + disengagement)\n",
        "    cf.MONTHS_SINCE_LAST_PURCHASE * COALESCE(cf.LOGIN_COUNT_30D, 0) AS MONTHS_X_LOGIN,\n",
        "    \n",
        "    -- Months since purchase * Cart abandonments (captures purchase delay + price sensitivity)\n",
        "    cf.MONTHS_SINCE_LAST_PURCHASE * COALESCE(cf.CART_ABANDONMENTS_30D, 0) AS MONTHS_X_ABANDON,\n",
        "    \n",
        "    -- Login count * Cart abandonments (captures engagement vs price sensitivity)\n",
        "    COALESCE(cf.LOGIN_COUNT_30D, 0) * COALESCE(cf.CART_ABANDONMENTS_30D, 0) AS LOGIN_X_ABANDON,\n",
        "    \n",
        "    -- Months since purchase * Email open rate (captures purchase delay + email engagement)\n",
        "    cf.MONTHS_SINCE_LAST_PURCHASE * COALESCE(cf.EMAIL_OPEN_RATE_30D, 0) AS MONTHS_X_EMAIL,\n",
        "    \n",
        "    -- Login count * Email open rate (captures overall engagement)\n",
        "    COALESCE(cf.LOGIN_COUNT_30D, 0) * COALESCE(cf.EMAIL_OPEN_RATE_30D, 0) AS LOGIN_X_EMAIL,\n",
        "    \n",
        "    -- ============================================\n",
        "    -- RATIO FEATURES\n",
        "    -- ============================================\n",
        "    -- Purchase frequency (orders per month since last purchase)\n",
        "    CASE \n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE > 0 THEN \n",
        "            COALESCE(cf.ORDER_COUNT_24M, 0) / NULLIF(cf.MONTHS_SINCE_LAST_PURCHASE, 0)\n",
        "        ELSE 0\n",
        "    END AS PURCHASE_FREQUENCY,\n",
        "    \n",
        "    -- Cart abandonment rate\n",
        "    CASE \n",
        "        WHEN COALESCE(cf.CART_ADDITIONS_30D, 0) > 0 THEN \n",
        "            COALESCE(cf.CART_ABANDONMENTS_30D, 0) / NULLIF(cf.CART_ADDITIONS_30D, 0)\n",
        "        ELSE 0\n",
        "    END AS CART_ABANDON_RATE,\n",
        "    \n",
        "    -- Login frequency (logins per month)\n",
        "    COALESCE(cf.LOGIN_COUNT_30D, 0) / 30.0 AS LOGIN_FREQUENCY,\n",
        "    \n",
        "    -- Return rate\n",
        "    CASE \n",
        "        WHEN COALESCE(cf.ORDER_COUNT_24M, 0) > 0 THEN \n",
        "            COALESCE(cf.TOTAL_RETURNS_COUNT, 0) / NULLIF(cf.ORDER_COUNT_24M, 0)\n",
        "        ELSE 0\n",
        "    END AS RETURN_RATE,\n",
        "    \n",
        "    -- ============================================\n",
        "    -- COMPOSITE ENGAGEMENT SCORES\n",
        "    -- ============================================\n",
        "    -- Overall engagement score (weighted combination)\n",
        "    (COALESCE(cf.EMAIL_OPEN_RATE_30D, 0) * 0.3 + \n",
        "     LEAST(COALESCE(cf.LOGIN_COUNT_30D, 0) / 30.0, 1.0) * 0.3 +\n",
        "     CASE \n",
        "         WHEN COALESCE(cf.CART_ADDITIONS_30D, 0) > 0 THEN \n",
        "             1 - LEAST(COALESCE(cf.CART_ABANDONMENTS_30D, 0) / NULLIF(cf.CART_ADDITIONS_30D, 0), 1.0)\n",
        "         ELSE 0\n",
        "     END * 0.2 +\n",
        "     CASE \n",
        "         WHEN cf.MONTHS_SINCE_LAST_PURCHASE > 0 THEN \n",
        "             LEAST(1.0 / NULLIF(cf.MONTHS_SINCE_LAST_PURCHASE, 0), 1.0)\n",
        "         ELSE 0\n",
        "     END * 0.2) AS ENGAGEMENT_SCORE,\n",
        "    \n",
        "    -- Purchase engagement score\n",
        "    (CASE \n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE <= 0.5 THEN 1.0\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE <= 1.0 THEN 0.7\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE <= 1.5 THEN 0.4\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE <= 2.0 THEN 0.2\n",
        "        ELSE 0.1\n",
        "     END * \n",
        "     LEAST(COALESCE(cf.ORDER_COUNT_24M, 0) / 10.0, 1.0)) AS PURCHASE_ENGAGEMENT,\n",
        "    \n",
        "    -- ============================================\n",
        "    -- RISK INDICATORS (Binary flags)\n",
        "    -- ============================================\n",
        "    -- High risk: inactive + low engagement\n",
        "    CASE \n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE >= 1.5 \n",
        "             AND COALESCE(cf.LOGIN_COUNT_30D, 0) <= 3 THEN 1 \n",
        "        ELSE 0 \n",
        "    END AS HIGH_RISK_INACTIVE,\n",
        "    \n",
        "    -- Disengagement: cart abandons + low email\n",
        "    CASE \n",
        "        WHEN COALESCE(cf.CART_ABANDONMENTS_30D, 0) >= 3 \n",
        "             AND COALESCE(cf.EMAIL_OPEN_RATE_30D, 0) < 0.2 THEN 1 \n",
        "        ELSE 0 \n",
        "    END AS HIGH_RISK_DISENGAGED,\n",
        "    \n",
        "    -- Price sensitivity: high abandons + low purchases\n",
        "    CASE \n",
        "        WHEN COALESCE(cf.CART_ABANDONMENTS_30D, 0) >= 3 \n",
        "             AND cf.MONTHS_SINCE_LAST_PURCHASE >= 1.0 THEN 1 \n",
        "        ELSE 0 \n",
        "    END AS HIGH_RISK_PRICE_SENSITIVE,\n",
        "    \n",
        "    -- ============================================\n",
        "    -- TEMPORAL TRENDS\n",
        "    -- ============================================\n",
        "    -- Recent activity decline (last 30 days vs 24 months)\n",
        "    CASE \n",
        "        WHEN COALESCE(cf.ORDER_COUNT_24M, 0) > 0 THEN \n",
        "            (COALESCE(cf.ORDER_COUNT_24M, 0) / 24.0) - (COALESCE(cf.LOGIN_COUNT_30D, 0) / 30.0)\n",
        "        ELSE 0\n",
        "    END AS ACTIVITY_DECLINE,\n",
        "    \n",
        "    -- Customer lifecycle stage (based on age and purchase frequency)\n",
        "    CASE \n",
        "        WHEN cf.CUSTOMER_AGE_MONTHS < 3 THEN 'New'\n",
        "        WHEN cf.CUSTOMER_AGE_MONTHS < 12 AND COALESCE(cf.ORDER_COUNT_24M, 0) >= 5 THEN 'Growing'\n",
        "        WHEN cf.CUSTOMER_AGE_MONTHS >= 12 AND COALESCE(cf.ORDER_COUNT_24M, 0) >= 10 THEN 'Mature'\n",
        "        WHEN cf.MONTHS_SINCE_LAST_PURCHASE >= 2 THEN 'Dormant'\n",
        "        ELSE 'Active'\n",
        "    END AS LIFECYCLE_STAGE\n",
        "    \n",
        "FROM OML.CHURN_FEATURES cf;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac495886",
      "metadata": {
        "oml_type": "script"
      },
      "outputs": [],
      "source": [
        "%script\n",
        "\n",
        "-- Create enhanced training data view\n",
        "CREATE OR REPLACE VIEW OML.CHURN_TRAINING_DATA_ENHANCED AS\n",
        "SELECT \n",
        "    cfe.*,\n",
        "    ctd.CHURNED_60_90D\n",
        "FROM OML.CHURN_FEATURES_ENHANCED cfe\n",
        "INNER JOIN OML.CHURN_TRAINING_DATA ctd ON cfe.USER_ID = ctd.USER_ID\n",
        "WHERE ctd.CUSTOMER_AGE_MONTHS >= 3\n",
        "    AND ctd.ORDER_COUNT_24M > 0;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "094f9226",
      "metadata": {
        "oml_type": "script"
      },
      "outputs": [],
      "source": [
        "%script\n",
        "\n",
        "-- Verify enhanced views\n",
        "SELECT COUNT(*) AS ENHANCED_FEATURES_COUNT FROM OML.CHURN_FEATURES_ENHANCED;\n",
        "SELECT COUNT(*) AS ENHANCED_TRAINING_COUNT FROM OML.CHURN_TRAINING_DATA_ENHANCED;\n",
        "\n",
        "-- Check new feature columns\n",
        "SELECT COLUMN_NAME \n",
        "FROM ALL_TAB_COLUMNS \n",
        "WHERE OWNER = 'OML' \n",
        "  AND TABLE_NAME = 'CHURN_FEATURES_ENHANCED'\n",
        "  AND (COLUMN_NAME LIKE 'MONTHS_X_%' \n",
        "       OR COLUMN_NAME LIKE 'LOGIN_X_%'\n",
        "       OR COLUMN_NAME LIKE '%_RATE'\n",
        "       OR COLUMN_NAME LIKE '%_SCORE'\n",
        "       OR COLUMN_NAME LIKE 'HIGH_RISK_%'\n",
        "       OR COLUMN_NAME LIKE 'PURCHASE_%'\n",
        "       OR COLUMN_NAME LIKE 'ACTIVITY_%'\n",
        "       OR COLUMN_NAME LIKE 'LIFECYCLE_%'\n",
        "       OR COLUMN_NAME LIKE 'ENGAGEMENT_%')\n",
        "ORDER BY COLUMN_NAME;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565dfc08",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Prepare features and split data (including categorical features)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load training data\n",
        "train_data = oml.sync(view='CHURN_TRAINING_DATA_ENHANCED')\n",
        "train_data_pd = train_data.pull()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Preparing Features and Splitting Data (Enhanced)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Training data shape: \" + str(train_data_pd.shape))\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = ['CUST_MARITAL_STATUS', 'CUST_INCOME_LEVEL', 'GENDER', \n",
        "                    'EDUCATION', 'OCCUPATION', 'HOUSEHOLD_SIZE', 'LOGIN_FREQUENCY_CATEGORY']\n",
        "\n",
        "# Convert categorical features to numeric codes\n",
        "for col in categorical_cols:\n",
        "    if col in train_data_pd.columns:\n",
        "        # Convert to category codes (numeric)\n",
        "        train_data_pd[col + '_NUM'] = pd.Categorical(train_data_pd[col]).codes\n",
        "        # Replace -1 (for NaN) with 0\n",
        "        train_data_pd[col + '_NUM'] = train_data_pd[col + '_NUM'].replace(-1, 0)\n",
        "        print(\"Converted \" + col + \" to \" + col + \"_NUM\")\n",
        "\n",
        "# Handle LIFECYCLE_STAGE if it exists\n",
        "if 'LIFECYCLE_STAGE' in train_data_pd.columns:\n",
        "    lifecycle_mapping = {'New': 0, 'Growing': 1, 'Mature': 2, 'Active': 3, 'Dormant': 4}\n",
        "    train_data_pd['LIFECYCLE_STAGE_NUM'] = train_data_pd['LIFECYCLE_STAGE'].map(lifecycle_mapping).fillna(0)\n",
        "    print(\"Converted LIFECYCLE_STAGE to LIFECYCLE_STAGE_NUM\")\n",
        "\n",
        "# Identify feature columns (exclude target, metadata, and original categorical columns)\n",
        "exclude_cols = ['USER_ID', 'CUSTOMER_SEGMENT', 'ESTIMATED_LTV', 'CHURNED_60_90D', 'LIFECYCLE_STAGE']\n",
        "# Also exclude original categorical columns (we'll use _NUM versions)\n",
        "exclude_cols.extend(categorical_cols)\n",
        "\n",
        "# Get all numeric columns (including converted categoricals)\n",
        "feature_cols = [col for col in train_data_pd.columns \n",
        "                if col not in exclude_cols\n",
        "                and pd.api.types.is_numeric_dtype(train_data_pd[col])]\n",
        "\n",
        "print(\"\\nTotal features: \" + str(len(feature_cols)))\n",
        "print(\"Features: \" + \", \".join(feature_cols[:10]) + \"...\")\n",
        "\n",
        "# Prepare X and y\n",
        "X_enhanced_pd = train_data_pd[feature_cols].copy()\n",
        "y_enhanced_pd = train_data_pd['CHURNED_60_90D']\n",
        "\n",
        "# Clean data - replace NaN and infinity\n",
        "for col in feature_cols:\n",
        "    if pd.api.types.is_numeric_dtype(X_enhanced_pd[col]):\n",
        "        X_enhanced_pd[col] = X_enhanced_pd[col].replace([np.inf, -np.inf], np.nan)\n",
        "        X_enhanced_pd[col] = X_enhanced_pd[col].fillna(0)\n",
        "\n",
        "# Stratified split\n",
        "X_train_enhanced_pd, X_test_enhanced_pd, y_train_enhanced_pd, y_test_enhanced_pd = train_test_split(\n",
        "    X_enhanced_pd, y_enhanced_pd, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_enhanced_pd\n",
        ")\n",
        "\n",
        "print(\"\\nSplit completed:\")\n",
        "print(\"  Train size: \" + str(len(X_train_enhanced_pd)))\n",
        "print(\"  Test size: \" + str(len(X_test_enhanced_pd)))\n",
        "print(\"  Train churn rate: \" + str(round(y_train_enhanced_pd.mean() * 100, 2)) + \"%\")\n",
        "print(\"  Test churn rate: \" + str(round(y_test_enhanced_pd.mean() * 100, 2)) + \"%\")\n",
        "print(\"  Total features: \" + str(len(feature_cols)))\n",
        "print(\"  Categorical features (converted): \" + str(len([f for f in feature_cols if '_NUM' in f])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "427e301d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Train XGBoost model with enhanced features (including categorical features)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training Enhanced XGBoost Model (with Categorical Features)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Merge X_train and y_train for database push\n",
        "train_enhanced_combined_pd = X_train_enhanced_pd.copy()\n",
        "train_enhanced_combined_pd['CHURNED_60_90D'] = y_train_enhanced_pd.values\n",
        "\n",
        "# Final check before pushing\n",
        "print(\"Final data validation before push:\")\n",
        "print(\"  Shape: \" + str(train_enhanced_combined_pd.shape))\n",
        "print(\"  Columns: \" + str(len(train_enhanced_combined_pd.columns)))\n",
        "print(\"  NULL count: \" + str(train_enhanced_combined_pd.isna().sum().sum()))\n",
        "\n",
        "# Ensure all feature columns are numeric\n",
        "for col in feature_cols:\n",
        "    if col in train_enhanced_combined_pd.columns:\n",
        "        train_enhanced_combined_pd[col] = pd.to_numeric(train_enhanced_combined_pd[col], errors='coerce').fillna(0)\n",
        "\n",
        "# Push to database\n",
        "print(\"\\nPushing enhanced training data to database...\")\n",
        "try:\n",
        "    train_enhanced_oml = oml.push(train_enhanced_combined_pd)\n",
        "    print(\"Training data pushed: \" + str(train_enhanced_oml.shape))\n",
        "except Exception as e:\n",
        "    print(\"Error pushing data: \" + str(e))\n",
        "    print(\"Trying with explicit data type conversion...\")\n",
        "    # Try converting all to float\n",
        "    for col in train_enhanced_combined_pd.columns:\n",
        "        if col != 'CHURNED_60_90D':\n",
        "            train_enhanced_combined_pd[col] = train_enhanced_combined_pd[col].astype(float)\n",
        "    train_enhanced_oml = oml.push(train_enhanced_combined_pd)\n",
        "    print(\"Training data pushed after type conversion: \" + str(train_enhanced_oml.shape))\n",
        "\n",
        "# Get actual columns available in OML DataFrame\n",
        "available_oml_cols = list(train_enhanced_oml.columns)\n",
        "print(\"\\nAvailable columns in OML DataFrame: \" + str(len(available_oml_cols)))\n",
        "\n",
        "# Define feature columns - use only features that exist in OML DataFrame\n",
        "enhanced_feature_cols_clean = [col for col in feature_cols if col in available_oml_cols]\n",
        "\n",
        "# Check for any missing features\n",
        "missing_features = [col for col in feature_cols if col not in available_oml_cols]\n",
        "if len(missing_features) > 0:\n",
        "    print(\"\\n⚠️  Warning: Some features missing from OML DataFrame:\")\n",
        "    print(\"  Missing: \" + \", \".join(missing_features[:5]) + \"...\")\n",
        "    print(\"  Using \" + str(len(enhanced_feature_cols_clean)) + \" available features\")\n",
        "\n",
        "print(\"\\nFeatures for training: \" + str(len(enhanced_feature_cols_clean)))\n",
        "categorical_count = len([f for f in enhanced_feature_cols_clean if '_NUM' in f])\n",
        "print(\"  - Categorical features (converted): \" + str(categorical_count))\n",
        "print(\"  - Numeric features: \" + str(len(enhanced_feature_cols_clean) - categorical_count))\n",
        "print(\"First 10 features: \" + \", \".join(enhanced_feature_cols_clean[:10]))\n",
        "\n",
        "# Create XGBoost model\n",
        "xgb_model_enhanced = oml.xgb('classification')\n",
        "\n",
        "# Get features and target from OML DataFrame\n",
        "X_train_enhanced_oml = train_enhanced_oml[enhanced_feature_cols_clean]\n",
        "y_train_enhanced_oml = train_enhanced_oml['CHURNED_60_90D']\n",
        "\n",
        "print(\"\\nX_train_enhanced_oml shape: \" + str(X_train_enhanced_oml.shape))\n",
        "print(\"Features: \" + str(len(enhanced_feature_cols_clean)))\n",
        "print(\"Training started...\")\n",
        "\n",
        "# Fit the model\n",
        "try:\n",
        "    xgb_model_enhanced = xgb_model_enhanced.fit(X_train_enhanced_oml, y_train_enhanced_oml)\n",
        "    print(\"Training completed!\")\n",
        "except Exception as e:\n",
        "    print(\"Training failed: \" + str(e))\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436639f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Evaluate enhanced model performance (with same features as training)\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Prepare test data - use same categorical conversion as training\n",
        "print(\"Preparing test data...\")\n",
        "test_enhanced_combined_pd = X_test_enhanced_pd.copy()\n",
        "test_enhanced_combined_pd['CHURNED_60_90D'] = y_test_enhanced_pd.values\n",
        "\n",
        "# Convert categorical features to numeric codes (same as training)\n",
        "categorical_cols = ['CUST_MARITAL_STATUS', 'CUST_INCOME_LEVEL', 'GENDER', \n",
        "                    'EDUCATION', 'OCCUPATION', 'HOUSEHOLD_SIZE', 'LOGIN_FREQUENCY_CATEGORY']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in test_enhanced_combined_pd.columns:\n",
        "        # Convert to category codes (numeric) - use same categories as training\n",
        "        # Get categories from training data to ensure consistency\n",
        "        if col in X_train_enhanced_pd.columns:\n",
        "            # Use the same categories from training\n",
        "            train_categories = pd.Categorical(X_train_enhanced_pd[col]).categories\n",
        "            test_enhanced_combined_pd[col + '_NUM'] = pd.Categorical(test_enhanced_combined_pd[col], categories=train_categories).codes\n",
        "        else:\n",
        "            test_enhanced_combined_pd[col + '_NUM'] = pd.Categorical(test_enhanced_combined_pd[col]).codes\n",
        "        # Replace -1 (for NaN) with 0\n",
        "        test_enhanced_combined_pd[col + '_NUM'] = test_enhanced_combined_pd[col + '_NUM'].replace(-1, 0)\n",
        "\n",
        "# Handle LIFECYCLE_STAGE if it exists\n",
        "if 'LIFECYCLE_STAGE' in test_enhanced_combined_pd.columns:\n",
        "    lifecycle_mapping = {'New': 0, 'Growing': 1, 'Mature': 2, 'Active': 3, 'Dormant': 4}\n",
        "    test_enhanced_combined_pd['LIFECYCLE_STAGE_NUM'] = test_enhanced_combined_pd['LIFECYCLE_STAGE'].map(lifecycle_mapping).fillna(0)\n",
        "\n",
        "# Clean all numeric columns\n",
        "for col in test_enhanced_combined_pd.columns:\n",
        "    if col != 'CHURNED_60_90D':\n",
        "        if pd.api.types.is_numeric_dtype(test_enhanced_combined_pd[col]):\n",
        "            test_enhanced_combined_pd[col] = pd.to_numeric(test_enhanced_combined_pd[col], errors='coerce').fillna(0)\n",
        "\n",
        "# Push to database\n",
        "print(\"Pushing test data to database...\")\n",
        "test_enhanced_oml = oml.push(test_enhanced_combined_pd)\n",
        "\n",
        "# Get actual columns available in OML DataFrame\n",
        "available_test_cols = list(test_enhanced_oml.columns)\n",
        "\n",
        "# Use only the features that were used in training (from Step 5)\n",
        "features_for_prediction = [col for col in enhanced_feature_cols_clean if col in available_test_cols]\n",
        "\n",
        "# Check for any missing features\n",
        "missing_features = [col for col in enhanced_feature_cols_clean if col not in available_test_cols]\n",
        "if len(missing_features) > 0:\n",
        "    print(\"⚠️  Warning: Some training features missing in test data:\")\n",
        "    print(\"  Missing: \" + \", \".join(missing_features))\n",
        "    print(\"  Using \" + str(len(features_for_prediction)) + \" available features\")\n",
        "\n",
        "print(\"Features for prediction: \" + str(len(features_for_prediction)))\n",
        "print(\"  (Same features as training)\")\n",
        "\n",
        "# Get features for prediction\n",
        "X_test_enhanced_oml = test_enhanced_oml[features_for_prediction]\n",
        "\n",
        "print(\"X_test_enhanced_oml shape: \" + str(X_test_enhanced_oml.shape))\n",
        "print(\"Features: \" + str(len(features_for_prediction)))\n",
        "\n",
        "# Get predictions\n",
        "print(\"\\nGenerating predictions...\")\n",
        "y_pred_proba_enhanced_oml = xgb_model_enhanced.predict_proba(X_test_enhanced_oml)\n",
        "\n",
        "# Convert to numpy array\n",
        "y_pred_proba_enhanced_pd = y_pred_proba_enhanced_oml.pull()\n",
        "if isinstance(y_pred_proba_enhanced_pd, pd.DataFrame):\n",
        "    if 1 in y_pred_proba_enhanced_pd.columns:\n",
        "        y_pred_proba_enhanced = y_pred_proba_enhanced_pd[1].values\n",
        "    elif len(y_pred_proba_enhanced_pd.columns) == 2:\n",
        "        y_pred_proba_enhanced = y_pred_proba_enhanced_pd.iloc[:, 1].values\n",
        "    else:\n",
        "        y_pred_proba_enhanced = y_pred_proba_enhanced_pd.values.flatten()\n",
        "else:\n",
        "    y_pred_proba_enhanced = np.array(y_pred_proba_enhanced_pd)\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.1)\n",
        "y_pred_enhanced = (y_pred_proba_enhanced >= 0.1).astype(int)\n",
        "y_test_enhanced_vals = y_test_enhanced_pd.values\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_enhanced = accuracy_score(y_test_enhanced_vals, y_pred_enhanced)\n",
        "precision_enhanced = precision_score(y_test_enhanced_vals, y_pred_enhanced, zero_division=0)\n",
        "recall_enhanced = recall_score(y_test_enhanced_vals, y_pred_enhanced, zero_division=0)\n",
        "f1_enhanced = f1_score(y_test_enhanced_vals, y_pred_enhanced, zero_division=0)\n",
        "auc_enhanced = roc_auc_score(y_test_enhanced_vals, y_pred_proba_enhanced)\n",
        "\n",
        "# Confusion matrix\n",
        "cm_enhanced = confusion_matrix(y_test_enhanced_vals, y_pred_enhanced)\n",
        "tn_enhanced, fp_enhanced, fn_enhanced, tp_enhanced = cm_enhanced.ravel()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ENHANCED MODEL EVALUATION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Accuracy:  \" + str(round(accuracy_enhanced, 4)) + \" (\" + str(round(accuracy_enhanced*100, 2)) + \"%)\")\n",
        "print(\"Precision: \" + str(round(precision_enhanced, 4)) + \" (\" + str(round(precision_enhanced*100, 2)) + \"%)\")\n",
        "print(\"Recall:    \" + str(round(recall_enhanced, 4)) + \" (\" + str(round(recall_enhanced*100, 2)) + \"%)\")\n",
        "print(\"F1 Score:  \" + str(round(f1_enhanced, 4)))\n",
        "print(\"AUC-ROC:   \" + str(round(auc_enhanced, 4)))\n",
        "print(\"Model Confidence: \" + str(int(auc_enhanced * 100)) + \"%\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"                Predicted\")\n",
        "print(\"              Non-Churn  Churn\")\n",
        "print(\"Actual Non-Churn   \" + str(tn_enhanced) + \"   \" + str(fp_enhanced))\n",
        "print(\"       Churn       \" + str(fn_enhanced) + \"   \" + str(tp_enhanced))\n",
        "\n",
        "# Compare with original model\n",
        "if 'auc' in globals() and 'f1' in globals():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"COMPARISON: Original vs Enhanced Model\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Metric              Original    Enhanced    Improvement\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    acc_improvement = accuracy_enhanced - accuracy\n",
        "    prec_improvement = precision_enhanced - precision\n",
        "    rec_improvement = recall_enhanced - recall\n",
        "    f1_improvement = f1_enhanced - f1\n",
        "    auc_improvement = auc_enhanced - auc\n",
        "    \n",
        "    print(\"Accuracy:           \" + str(round(accuracy, 4)) + \"      \" + str(round(accuracy_enhanced, 4)) + \"      \" + str(round(acc_improvement, 4)))\n",
        "    print(\"Precision:          \" + str(round(precision, 4)) + \"      \" + str(round(precision_enhanced, 4)) + \"      \" + str(round(prec_improvement, 4)))\n",
        "    print(\"Recall:             \" + str(round(recall, 4)) + \"      \" + str(round(recall_enhanced, 4)) + \"      \" + str(round(rec_improvement, 4)))\n",
        "    print(\"F1 Score:           \" + str(round(f1, 4)) + \"      \" + str(round(f1_enhanced, 4)) + \"      \" + str(round(f1_improvement, 4)))\n",
        "    print(\"AUC-ROC:            \" + str(round(auc, 4)) + \"      \" + str(round(auc_enhanced, 4)) + \"      \" + str(round(auc_improvement, 4)))\n",
        "    \n",
        "    improvement_pct = ((auc_enhanced - auc) / auc * 100) if auc > 0 else 0\n",
        "    print(\"\\nAUC Improvement: \" + str(round(improvement_pct, 2)) + \"%\")\n",
        "    \n",
        "    if auc_improvement > 0.01:\n",
        "        print(\"✓ Significant improvement! Feature engineering is working.\")\n",
        "    elif auc_improvement > 0:\n",
        "        print(\"✓ Small improvement. May need more feature engineering or hyperparameter tuning.\")\n",
        "    else:\n",
        "        print(\"⚠️  No improvement. Consider:\")\n",
        "        print(\"   - Different interaction features\")\n",
        "        print(\"   - Hyperparameter tuning\")\n",
        "        print(\"   - Trying other algorithms\")\n",
        "else:\n",
        "    print(\"\\n(Original model metrics not available for comparison)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6410ec34",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Get feature importance for enhanced model\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Enhanced Model Feature Importance\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get importance\n",
        "importance_enhanced_result = xgb_model_enhanced.importance\n",
        "importance_enhanced_df_raw = importance_enhanced_result.pull()\n",
        "\n",
        "# Extract feature names and importance\n",
        "if 'ATTRIBUTE_NAME' in importance_enhanced_df_raw.columns and 'GAIN' in importance_enhanced_df_raw.columns:\n",
        "    importance_enhanced_df = importance_enhanced_df_raw[['ATTRIBUTE_NAME', 'GAIN']].copy()\n",
        "    importance_enhanced_df.columns = ['FEATURE_NAME', 'IMPORTANCE_SCORE']\n",
        "    importance_enhanced_df = importance_enhanced_df.sort_values('IMPORTANCE_SCORE', ascending=False)\n",
        "    \n",
        "    print(\"\\nTop 20 Most Important Features (Enhanced Model):\")\n",
        "    print(importance_enhanced_df.head(20).to_string(index=False))\n",
        "    \n",
        "    # Check if new engineered features are in top features\n",
        "    new_features_list = [\n",
        "        'MONTHS_X_LOGIN', 'MONTHS_X_ABANDON', 'LOGIN_X_ABANDON', \n",
        "        'MONTHS_X_EMAIL', 'LOGIN_X_EMAIL',\n",
        "        'PURCHASE_FREQUENCY', 'CART_ABANDON_RATE', 'RETURN_RATE',\n",
        "        'ENGAGEMENT_SCORE', 'PURCHASE_ENGAGEMENT',\n",
        "        'HIGH_RISK_INACTIVE', 'HIGH_RISK_DISENGAGED', 'HIGH_RISK_PRICE_SENSITIVE',\n",
        "        'ACTIVITY_DECLINE'\n",
        "    ]\n",
        "    \n",
        "    top_features = importance_enhanced_df.head(20)['FEATURE_NAME'].tolist()\n",
        "    new_features_in_top = [f for f in top_features if f in new_features_list]\n",
        "    \n",
        "    print(\"\\nNew engineered features in top 20: \" + str(len(new_features_in_top)))\n",
        "    if len(new_features_in_top) > 0:\n",
        "        print(\"Features: \" + \", \".join(new_features_in_top))\n",
        "        print(\"\\n✓ New engineered features are contributing to the model!\")\n",
        "    else:\n",
        "        print(\"No new engineered features in top 20\")\n",
        "        print(\"(This may indicate the original features are still more predictive)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844505d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Summary of feature engineering results\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n--- What Was Added ---\")\n",
        "print(\"1. Interaction Features:\")\n",
        "print(\"   - MONTHS_X_LOGIN, MONTHS_X_ABANDON, LOGIN_X_ABANDON\")\n",
        "print(\"   - MONTHS_X_EMAIL, LOGIN_X_EMAIL\")\n",
        "print(\"2. Ratio Features:\")\n",
        "print(\"   - PURCHASE_FREQUENCY, CART_ABANDON_RATE, RETURN_RATE\")\n",
        "print(\"3. Composite Scores:\")\n",
        "print(\"   - ENGAGEMENT_SCORE, PURCHASE_ENGAGEMENT\")\n",
        "print(\"4. Risk Indicators:\")\n",
        "print(\"   - HIGH_RISK_INACTIVE, HIGH_RISK_DISENGAGED, HIGH_RISK_PRICE_SENSITIVE\")\n",
        "print(\"5. Temporal Features:\")\n",
        "print(\"   - ACTIVITY_DECLINE, LIFECYCLE_STAGE\")\n",
        "print(\"6. Categorical Features:\")\n",
        "print(\"   - Converted to numeric codes for model inclusion\")\n",
        "\n",
        "print(\"\\n--- Model Performance ---\")\n",
        "if 'auc_enhanced' in globals() and 'auc' in globals():\n",
        "    print(\"Original Model AUC: \" + str(round(auc, 4)) + \" (\" + str(int(auc * 100)) + \"%)\")\n",
        "    print(\"Enhanced Model AUC: \" + str(round(auc_enhanced, 4)) + \" (\" + str(int(auc_enhanced * 100)) + \"%)\")\n",
        "    improvement = auc_enhanced - auc\n",
        "    improvement_pct = (improvement / auc * 100) if auc > 0 else 0\n",
        "    print(\"Improvement: \" + str(round(improvement, 4)) + \" (\" + str(round(improvement_pct, 2)) + \"%)\")\n",
        "    \n",
        "    if improvement > 0.01:\n",
        "        print(\"\\n✓ Significant improvement! Feature engineering is working.\")\n",
        "    elif improvement > 0:\n",
        "        print(\"\\n✓ Small improvement. May need more feature engineering or hyperparameter tuning.\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  No improvement. Consider:\")\n",
        "        print(\"   - Different interaction features\")\n",
        "        print(\"   - Hyperparameter tuning\")\n",
        "        print(\"   - Trying other algorithms\")\n",
        "\n",
        "print(\"\\n--- Next Steps ---\")\n",
        "print(\"1. If improved: Save enhanced model and use for production\")\n",
        "print(\"2. If not improved: Try hyperparameter tuning\")\n",
        "print(\"3. Consider: Additional feature engineering based on domain knowledge\")\n",
        "print(\"4. Validate: Test on holdout set or cross-validation\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Feature engineering complete!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
